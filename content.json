{"posts":[{"title":"Python爬虫入门","text":"一些Python爬虫入门的小案例 练习使用通用爬虫框架获取静态网页的文本内容。使用Requests、urllib库，获取 http://www.ip138.com/ip 归属地查询结果，并打印输出页面的HTML文本内容。 完整代码1234567import requestsheaders = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}# 获取ip归属地查询结果，并打印输出页面的HTML文本内容ip = requests.get(&quot;http://2022.ip138.com&quot;, headers=headers).textprint(ip) 练习使用通用爬虫框架获取多个静态网页的文本内容。使用Requests库，获取豆瓣电影TOP250 https://movie.douban.com/top250?start=0&amp;filter= 前10页查询结果，打印输出页面的HTML文本内容。同时，将第一页内容保存到txt文件中。 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869from bs4 import BeautifulSoupfrom lxml import htmlimport requestsimport os, csvdef write_dictionary_to_csv(dict, filename): file_exists = os.path.isfile(filename) with open(filename, 'a', encoding='utf-8') as f: w = csv.DictWriter(f, dict.keys(), delimiter=',', quotechar='&quot;', lineterminator='\\n', quoting=csv.QUOTE_ALL, skipinitialspace=True) if not file_exists: w.writeheader() w.writerow(dict) print('当前行写入csv成功！')rank = 1def write_one_page(url): headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'} html = requests.get(url, headers=headers).text # lxml：html解析库（把HTML代码转化成Python对象） soup = BeautifulSoup(html, 'lxml') global rank for k in soup.find('div', class_='article').find_all('div', class_='info'): name = k.find('div', class_='hd').find_all('span') # 电影名字 score = k.find('div', class_='star').find_all('span') # 分数 # 抓取年份、国家 actor_infos_html = k.find(class_='bd') actor_infos = actor_infos_html.find('p').get_text().strip().split('\\n') actor_infos1 = actor_infos[0].split('\\xa0\\xa0\\xa0') director = actor_infos1[0][3:] role = actor_infos[1] year_area = actor_infos[1].lstrip().split('\\xa0/\\xa0') year = year_area[0] country = year_area[1] type = year_area[2] print(rank, name[0].string, score[1].string, year, country, type) data = { 'rank': rank, 'name': name[0].string, 'score': score[1].string, 'year': year, 'country': country, 'type': type } # print(data) write_dictionary_to_csv(data, 'top250.csv')if __name__ == '__main__': headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'} # 爬取第一页HTML文本内容，并保存到txt文件中 html = requests.get(&quot;https://movie.douban.com/top250?start=0&amp;filter=&quot;, headers=headers).text print(html) with open('test.txt', 'w', encoding='utf-8') as f: f.write(html) # 前十页查询结果 for i in range(10): a = i * 25 url = &quot;https://movie.douban.com/top250?start=&quot; + str(a) + &quot;&amp;filter=&quot; write_one_page(url) 爬取豆瓣小说-随笔分类前5页内容，解析网页，将其中每一项的标题、作者、价格、评分、评价人数、简介以csv格式输出至文件中第一页网址：https://book.douban.com/tag/%E9%9A%8F%E7%AC%94 将1中获得的第一页的图书标题、作者、价格、评分、评价人数依次存入数据库dbread的book_list表中 完整代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import csvimport pymysqlimport requestsfrom bs4 import BeautifulSoupconnect = pymysql.Connect( host='localhost', port=3306, user='root', passwd='000000', db='dbread', charset='utf8mb4')# 写入表头数据file = open('豆瓣小说.csv', mode='a', encoding='utf-8', newline='')csv_write = csv.DictWriter(file, fieldnames=['标题', '作者', '价格', '评分', '评价人数', '简介'])csv_write.writeheader()for i in range(5): a = i * 20 url = 'https://book.douban.com/tag/%E9%9A%8F%E7%AC%94?start=&quot; + str(a) + &quot;&amp;type=T' headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36 Edg/106.0.1370.52', } html = requests.get(url=url, headers=headers).text soup = BeautifulSoup(html, &quot;html.parser&quot;) # 查找所需要的数据 divs = soup.find_all(&quot;div&quot;, class_=&quot;info&quot;) for div in divs: title = div.find(&quot;h2&quot;).get_text().strip(&quot;\\n /&quot;).replace(&quot; &quot;, &quot;&quot;).replace(&quot;\\n&quot;, &quot;&quot;) author = div.find(&quot;div&quot;, class_=&quot;pub&quot;).get_text().strip(&quot;\\n /&quot;) price_ = div.find(&quot;span&quot;, class_=&quot;buy-info&quot;) if price_ is not None: price = price_.get_text().strip(&quot;\\n /&quot;) else: price = &quot;&quot; score_ = div.find(&quot;span&quot;, class_=&quot;rating_nums&quot;) if score_ is not None: score = score_.get_text().strip(&quot;\\n /&quot;) else: score = &quot;&quot; pl = div.find(&quot;span&quot;, class_=&quot;pl&quot;).get_text().strip(&quot;\\n /&quot;).lstrip(&quot;(&quot;).rstrip(&quot;)&quot;) info = div.find(&quot;p&quot;).get_text().strip(&quot;\\n /&quot;).replace(&quot;\\n&quot;, &quot;&quot;) print(title, author, price, score, pl, info) # 写入csv文件 data_dict = {'标题': title, '作者': author, '价格': price, '评分': score, '评价人数': pl, '简介': info} csv_write.writerow(data_dict) # 写入数据库 cursor = connect.cursor() sql = &quot;INSERT INTO book_list(title, author, price, score, pl, info) VALUES('%s','%s','%s','%s','%s','%s')&quot; data = (title, author, price, score, pl, info) cursor.execute(sql % data) connect.commit() print('成功插入数据')connect.close() 运行截图 csv文件 保存到mysql数据库中的数据","link":"/Article/Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"title":"ADB调试","text":"连接方式1. USB 连接 通过 USB 连接来正常使用 adb 需要以下步骤： 确认硬件状态正常(包括 Android 设备处于正常开机状态，USB 连接线和各种接口完好)。 Android 设备的开发者选项和 USB 调试模式已开启(可以在「设置」-「开发者选项」-「USB调试」打开USB调试)。 确认设备驱动状态正常(安装ADB驱动程序)。 通过 USB 线连接好电脑和设备后确认状态。 通过 adb devices 命令查看设备连接情况。 2. WLAN 连接（需要 USB 线）借助 USB 通过 WiFi 连接来正常使用 adb 需要以下步骤：操作步骤： 将 Android 设备与要运行 adb 的电脑连接到同一个 WiFi。 将设备与电脑通过 USB 线连接(可通过 adb devices 命令查看设备连接情况)。 通过 adb tcpip 5555 命令让设备在 5555 端口监听 TCP/IP 连接。 断开 USB 连接。 找到设备的 IP 地址(可以在「设置」-「关于手机」-「状态信息」-「IP地址」查看 IP 地址)。 通过 adb connect &lt;device-ip-address&gt; 命令使用 IP 地址将 Android 设备与电脑连接。 通过 adb devices 命令查看设备连接情况。 使用完毕后可通过 adb disconnect &lt;device-ip-address&gt; 命令断开无线连接。 3. WLAN 连接（无需借助 USB 线）注：需要 root 权限。不借助 USB 通过 WiFi 连接来正常使用 adb 需要以下步骤： 在 Android 设备上安装一个终端模拟器(可通过Terminal Emulator for Android Downloads下载)。 将 Android 设备与要运行 adb 的电脑连接到同一个 WiFi。 打开 Android 设备上的终端模拟器，在里面依次运行命令：12susetprop service.adb.tcp.port 5555 找到设备的 IP 地址(可以在「设置」-「关于手机」-「状态信息」-「IP地址」查看 IP 地址)。 通过 adb connect &lt;device-ip-address&gt; 命令使用 IP 地址将 Android 设备与电脑连接。 通过 adb devices 命令查看设备连接情况。 4. WiFi 连接转为 USB 连接通过adb usb命令以USB模式重新启动ADB： 1adb usb 连接手机手机通过usb连接电脑在电脑输入 1adb devices 开启adb调试之后在电脑上输入 1adb shell 对电脑进行授权配置好adb的电脑，在C:\\Users\\用户名.android文件夹下有一个adbkey.pub文件，右键打开终端，将这个文件夹复制到手机的/data/misc/adb/文件夹下并且重命名为adb_keys具体操作如下： 1adb push adb_keys /data/misc/adb/adb_keys","link":"/Article/ADB%E8%B0%83%E8%AF%95/"},{"title":"Kafka入门","text":"实验目的1、熟悉kafka操作的常用命令2、熟练使用python编写kafka程序3、熟练完成kafka与MySQL的交互4、熟悉消费者订阅分区和手动提交偏移量的API 启动服务使用Kafka之前需要先启动一个ZooKeeper服务，直接使用Kafka中包含的脚本 1root@vm:/usr/local/kafka/bin# ./zookeeper-server-start.sh config/zookeeper.properties 新开一个终端，启动Kafka服务 1root@vm:/usr/local/kafka/bin# ./kafka-server-start.sh config/server.properties 1． Kafka与MySQL的组合使用读取student表的数据内容，将其转为JSON格式，发送给Kafka 从Kafka中获取Json格式数据，打印出来 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# producerfrom kafka import KafkaProducerimport jsonimport pymysqldef mysql(): # 连接数据库 connect = pymysql.Connect( host='192.168.234.131', port=3306, user='root', # 数据库用户名 passwd='password', # 密码 db='school', charset='utf8' ) # 获取游标对象 cursor = connect.cursor() # 查询数据 sql = &quot;select * from student&quot; # 执行SQL语句 result = cursor.execute(sql) # fetchall查询所有结果 data = cursor.fetchall() for msg in data: val = {} val['sno'] = msg[0] val['sname'] = msg[1] val['ssex'] = msg[2] val['sage'] = msg[3] producer.send('json_topic', val) # 发送的topic为json # 提交事务 connect.commit() print(val) connect.close()if __name__ == '__main__': producer = KafkaProducer(bootstrap_servers='192.168.234.131:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8'), api_version=(0, 10)) # 连接kafka mysql() producer.close()# consumerimport jsonfrom kafka import KafkaConsumerfrom kafka.structs import TopicPartitionconsumer = KafkaConsumer('json_topic', bootstrap_servers=[ '192.168.234.131:9092'], group_id=None, auto_offset_reset='earliest', api_version=(0, 10))for message in consumer: msg1 = str(message.value, encoding=&quot;utf-8&quot;) dict = json.loads(msg1) print(&quot;%s:%d:%d:%s&quot; % (message.topic, message.partition, message.offset, dict)) 2． Kafka消费者手动提交生成data.json文件 123456789101112131415161718[ { &quot;name&quot;: &quot;Tony&quot;, &quot;age&quot;: &quot;21&quot;, &quot;hobbies&quot;: [ &quot;basketball&quot;, &quot;tennis&quot; ] }, { &quot;name&quot;: &quot;Lisa&quot;, &quot;age&quot;: &quot;20&quot;, &quot;hobbies&quot;: [ &quot;sing&quot;, &quot;dance&quot; ] }] 编写生产者程序，将JSON文件数据发送给Kafka 123456def data(): with open('data.json', 'r', encoding='utf8')as fp: data = json.load(fp) # 读取json文件 for dict in data: print(dict) producer.send('json_topic', dict) 编写消费者程序，读取Kafka的JSON格式数据，并重置偏移量，从第5个偏移量消费 123456# 重置偏移量，从第5个偏移量消费consumer.seek(TopicPartition(topic='json_topic', partition=0), 5)for message in consumer: msg1 = str(message.value, encoding=&quot;utf-8&quot;) dict = json.loads(msg1) print(&quot;%s:%d:%d:%s&quot; % (message.topic, message.partition, message.offset, dict)) 3. Kafka消费者订阅分区启动Kafka，手动创建主体“assign_topic”,分区数量为2 1root@vm:/usr/local/kafka/bin# ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 2 --topic assign_topic 编写生产者程序，以通用唯一标识符UUID作为消息，发送给主体”assign_topic” 12345678import uuidfrom kafka import KafkaProducerproducer = KafkaProducer( bootstrap_servers='192.168.234.131:9092', api_version=(0, 10)) # 连接kafkamsg = str(uuid.uuid1()).encode('utf-8') # 发送内容,必须是bytes类型producer.send('assign_topic', msg) # 发送的topic为testproducer.close() 编写消费者程序，订阅主题的分区0，只消费分区0数据 编写消费者程序，订阅主题的分区1，只消费分区1数据 12345678910111213141516from kafka import KafkaConsumerfrom kafka.structs import TopicPartitiondef main(partition): consumer = KafkaConsumer(bootstrap_servers=[ '192.168.234.131:9092'], group_id=None, auto_offset_reset='earliest', api_version=(0, 10)) consumer.assign([TopicPartition('assign_topic', partition)]) print(consumer.assignment()) # 获取当前消费者订阅的主题 for msg in consumer: recv = &quot;topic名称:%s 分区位置:%d 偏移量:%d key=%s value=%s&quot; % ( msg.topic, msg.partition, msg.offset, msg.key, msg.value) print(recv)if __name__ == '__main__': # main(partition=0) main(partition=1) 实验总结Kafka 是由 Linkedin 公司开发的，它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的分布式消息流平台，它同时也是一款开源的基于发布订阅模式的消息引擎系统，是一种高吞吐、低延迟的分布式发布订阅消息系统。","link":"/Article/Kafka%E5%85%A5%E9%97%A8/"},{"title":"Hadoop高可用集群搭建","text":"之前的博客写了搭建hadoop集群环境，今天写一写搭建高可用（HA）环境。Hadoop-HA模式大致分为两个(个人在学习中的理解)： namenode 高可用 yarn 高可用 一、NameNode HA 首先启动zookeeper集群 123456789101112131415161718192021222324252627282930#先启动三台zookeeper集群：[root@hadoop01 bin]# pwd/opt/module/zookeeper-3.4.10/bin[root@hadoop01 bin]# ./zkServer.sh start[root@hadoop02 bin]# pwd/opt/module/zookeeper-3.4.10/bin[root@hadoop02 bin]# ./zkServer.sh start[root@hadoop03 bin]# pwd/opt/module/zookeeper-3.4.10/bin[root@hadoop03 bin]# ./zkServer.sh start#分别查看三台状态[root@hadoop01 bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower[root@hadoop02 bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: leader[root@hadoop03 bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower# 启动成功 修改配置 修改core-site.xml 修改hdfs.site.xml 启动journalnode 12345678910111213[root@hadoop01 hadoop]# sbin/hadoop-daemon.sh start journalnodestarting journalnode, logging to /opt/module/hadoop/logs/hadoop-root-journalnode-hadoop01.out[root@hadoop01 hadoop]# jps5873 JournalNode5511 QuorumPeerMain5911 Jps[root@hadoop03 hadoop]# sbin/hadoop-daemon.sh start journalnodestarting journalnode, logging to /opt/module/hadoop/logs/hadoop-root-journalnode-hadoop03.out[root@hadoop03 hadoop]# jps5344 JournalNode5382 Jps5021 QuorumPeerMain 格式化NameNode并启动，也是只需要启动hadoop01和hadoop03 1234567891011121314151617# 第一个namenode格式化[root@hadoop01 hadoop]# bin/hdfs namenode -format# 格式化之后启动namenode[root@hadoop01 hadoop]# sbin/hadoop-daemon.sh start namenodestarting namenode, logging to /opt/module/hadoop/logs/hadoop-root-namenode-hadoop01.out[root@hadoop01 hadoop]# jps5511 QuorumPeerMain7000 Jps6591 JournalNode6895 NameNode# 第二个namenode同步第一个[root@hadoop03 hadoop]# bin/hdfs namenode -bootstrapStandby# 启动第二个namenode[root@hadoop03 hadoop]# sbin/hadoop-daemon.sh start namenode 查看namenode(如下图，都启动成功，只是都在standby状态) 手动切换nn1为激活状态 1234567891011121314151617181920212223242526[root@hadoop01 hadoop]# bin/hdfs haadmin -transitionToActive nn1Automatic failover is enabled for NameNode at hadoop03/192.168.170.133:8020Refusing to manually manage HA state, since it may causea split-brain scenario or other incorrect state.If you are very sure you know what you are doing, please specify the forcemanual flag.# 这里需要强制切换[root@hadoop01 hadoop]# bin/hdfs haadmin -transitionToActive --forcemanual nn1You have specified the forcemanual flag. This flag is dangerous, as it can induce a split-brain scenario that WILL CORRUPT your HDFS namespace, possibly irrecoverably.It is recommended not to use this flag, but instead to shut down the cluster and disable automatic failover if you prefer to manually manage your HA state.You may abort safely by answering 'n' or hitting ^C now.Are you sure you want to continue? (Y or N) y18/08/21 16:12:59 WARN ha.HAAdmin: Proceeding with manual HA state management even thoughautomatic failover is enabled for NameNode at hadoop03/192.168.170.133:802018/08/21 16:13:00 WARN ha.HAAdmin: Proceeding with manual HA state management even thoughautomatic failover is enabled for NameNode at hadoop01/192.168.170.131:8020# 可以看到nn2已经激活，nn1在standby状态[root@hadoop01 hadoop]# bin/hdfs haadmin -getServiceState nn1standby[root@hadoop01 hadoop]# bin/hdfs haadmin -getServiceState nn2active 在zookeeper上配置故障自动转移节点 1234[root@hadoop01 hadoop]# bin/hdfs zkfc -formatZK[zk: localhost:2181(CONNECTED) 5] ls /[zookeeper, hadoop-ha]# 可以看到zookeeper上已经有了hadoop-ha节点了 启动集群 1234567891011121314151617181920212223[root@hadoop01 hadoop]# sbin/start-dfs.shStarting namenodes on [hadoop01 hadoop03]hadoop01: namenode running as process 6895. Stop it first.hadoop03: namenode running as process 6103. Stop it first.hadoop03: starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-hadoop03.outhadoop01: starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-hadoop01.outhadoop02: starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-hadoop02.outStarting journal nodes [hadoop01 hadoop02 hadoop03]hadoop01: journalnode running as process 6591. Stop it first.hadoop03: journalnode running as process 5814. Stop it first.hadoop02: journalnode running as process 5450. Stop it first.Starting ZK Failover Controllers on NN hosts [hadoop01 hadoop03]hadoop01: starting zkfc, logging to /opt/module/hadoop/logs/hadoop-root-zkfc-hadoop01.outhadoop03: starting zkfc, logging to /opt/module/hadoop/logs/hadoop-root-zkfc-hadoop03.out[root@hadoop01 hadoop]# jps8114 DFSZKFailoverController7478 ZooKeeperMain5511 QuorumPeerMain8169 Jps7803 DataNode6591 JournalNode6895 NameNode# 在三台设备上分别jps一下，都启动了 接下来kill掉一个nn1，看看zookeeper是否会自动切换nn2 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@hadoop03 hadoop]# jps6708 DFSZKFailoverController6549 DataNode5814 JournalNode6103 NameNode6825 Jps5021 QuorumPeerMain# 杀掉namenode进程[root@hadoop03 hadoop]# kill -9 6103#查看nn2状态，连接不上了[root@hadoop03 hadoop]# bin/hdfs haadmin -getServiceState nn218/08/21 16:28:52 INFO ipc.Client: Retrying connect to server: hadoop03/192.168.170.133:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)Operation failed: Call From hadoop03/192.168.170.133 to hadoop03:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused# 查看nn1状态，已经激活了[root@hadoop03 hadoop]# bin/hdfs haadmin -getServiceState nn1active#重新启动同步nn1，并启动nn2[root@hadoop03 hadoop]# bin/hdfs namenode -bootstrapStandby[root@hadoop03 hadoop]# sbin/hadoop-daemon.sh start namenodestarting namenode, logging to /opt/module/hadoop/logs/hadoop-root-namenode-hadoop03.out[root@hadoop03 hadoop]# jps7169 Jps6708 DFSZKFailoverController6549 DataNode5814 JournalNode7084 NameNode5021 QuorumPeerMain# 接下来杀掉nn1，看看会不会自动激活nn2[root@hadoop01 hadoop]# jps8114 DFSZKFailoverController8418 Jps7478 ZooKeeperMain5511 QuorumPeerMain7803 DataNode6591 JournalNode6895 NameNode[root@hadoop01 hadoop]# kill -9 6895[root@hadoop01 hadoop]# bin/hdfs haadmin -getServiceState nn118/08/21 16:32:11 INFO ipc.Client: Retrying connect to server: hadoop01/192.168.170.131:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)Operation failed: Call From hadoop01/192.168.170.131 to hadoop01:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused[root@hadoop01 hadoop]# bin/hdfs haadmin -getServiceState nn2active NameNode-HA搭建完成！ 二、Yarn HA 修改配置 修改yarn-site.xml 启动yarn，由于hadoop02是resourceManager，所以在hadoop02上启动 12345678910111213141516171819202122232425262728293031[root@hadoop02 hadoop]# sbin/start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-hadoop02.outhadoop02: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-hadoop02.outhadoop01: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-hadoop01.outhadoop03: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-hadoop03.out[root@hadoop02 hadoop]# jps4898 QuorumPeerMain7352 ResourceManager5801 DataNode7449 NodeManager5450 JournalNode7482 Jps[root@hadoop01 hadoop]# jps8114 DFSZKFailoverController9508 Jps7478 ZooKeeperMain5511 QuorumPeerMain7803 DataNode9389 NodeManager6591 JournalNode[root@hadoop03 hadoop]# jps6708 DFSZKFailoverController6549 DataNode5814 JournalNode7961 Jps7084 NameNode7932 NodeManager5021 QuorumPeerMain 在hadoop03上启动resourceManager 1234567891011[root@hadoop03 hadoop]# sbin/yarn-daemon.sh start resourcemanagerstarting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-hadoop03.out[root@hadoop03 hadoop]# jps6708 DFSZKFailoverController6549 DataNode5814 JournalNode8105 ResourceManager7084 NameNode7932 NodeManager8140 Jps5021 QuorumPeerMain 查看两个resourceManager的状态 1234[root@hadoop03 hadoop]# bin/yarn rmadmin -getServiceState rm1active[root@hadoop03 hadoop]# bin/yarn rmadmin -getServiceState rm2standby 杀掉rm1，看看是否会自动切换rm2 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@hadoop02 hadoop]# jps4898 QuorumPeerMain7352 ResourceManager5801 DataNode7449 NodeManager5450 JournalNode7854 Jps# kill掉rm1[root@hadoop02 hadoop]# kill -9 7352# 查看rm1状态，已经离线了[root@hadoop02 hadoop]# bin/yarn rmadmin -getServiceState rm118/08/21 17:22:31 INFO ipc.Client: Retrying connect to server: hadoop02/192.168.170.132:8033. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)Operation failed: Call From hadoop02/192.168.170.132 to hadoop02:8033 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused# 查看rm2状态，激活了[root@hadoop02 hadoop]# bin/yarn rmadmin -getServiceState rm2active# 接下来重启rm1，kill掉rm2，看看是否会切换# 启动hadoop02的rm1[root@hadoop02 hadoop]# sbin/yarn-daemon.sh start resourcemanagerstarting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-hadoop02.out[root@hadoop02 hadoop]# jps4898 QuorumPeerMain5801 DataNode7449 NodeManager5450 JournalNode8091 Jps8046 ResourceManager# kill hadoop03的rm2[root@hadoop03 hadoop]# jps6708 DFSZKFailoverController6549 DataNode5814 JournalNode8503 Jps8105 ResourceManager7084 NameNode7932 NodeManager5021 QuorumPeerMain[root@hadoop03 hadoop]# kill 8105# 查看rm1和rm2的状态[root@hadoop03 hadoop]# bin/yarn rmadmin -getServiceState rm1active[root@hadoop03 hadoop]# bin/yarn rmadmin -getServiceState rm218/08/21 17:26:23 INFO ipc.Client: Retrying connect to server: hadoop03/192.168.170.133:8033. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)Operation failed: Call From hadoop03/192.168.170.133 to hadoop03:8033 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused# 可以看到，已经切换成功了 Yarn-HA搭建成功！","link":"/Article/HadoopHA/"},{"title":"Scrapy爬虫入门","text":"实验要求1.要求爬取贝壳新房济南 https://jn.zu.ke.com/zufang 挂牌出租的全部楼盘信息。爬取信息包括楼盘名称、链接、地址、大小、方向、居室、价格、姓名。2.实现多页爬取（多次请求解析）：start_urls作为请求的入口地址，类型为列表，可以在列表中追加我们想要爬取页面的url地址。还需要在爬虫程序中添加scrapy.Request(url[,callback,method=“GET”,headers,body,cookies,meta,dont_filter=False])3.爬取5页中所有要求的数据，起始页从学号尾数开始，存入csv中 安装Scrapy框架通过pip安装scrapy： 1pip install Scrapy 创建一个新的scrapy项目： 1scrapy startproject Beike 执行完命令生成的文件树如下图所示 完善item.py文件1234567891011class BeikeItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() link = scrapy.Field() address = scrapy.Field() big = scrapy.Field() where = scrapy.Field() how = scrapy.Field() price = scrapy.Field() name = scrapy.Field() 在setting.py中设置UA1USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.35' 编写爬虫文件123456789101112131415161718192021222324252627282930313233343536373839class BeikeSpider(scrapy.Spider): name = 'beike' allowed_domains = ['jn.zu.ke.com'] start_urls = ['https://jn.zu.ke.com/zufang'] page = 20 def parse(self, response): print(response.url) node_list = response.xpath('//div[@class=&quot;content__list--item--main&quot;]') print(len(node_list)) item = BeikeItem() for node in node_list: item[&quot;title&quot;] = node.xpath(&quot;./p[1]/a/text()&quot;).get().strip().strip() item[&quot;link&quot;] = response.urljoin(node.xpath(&quot;./p[1]/a/@href&quot;).get().strip()) item[&quot;address&quot;] = node.xpath(&quot;./p[2]/a[3]/text()&quot;).get() item[&quot;big&quot;] = node.xpath(&quot;./p[2]/text()[last()-3]&quot;).get() item[&quot;where&quot;] = node.xpath(&quot;./p[2]/text()[last()-2]&quot;).get() item[&quot;how&quot;] = node.xpath(&quot;./p[2]/text()[last()-1]&quot;).get().strip() item[&quot;price&quot;] = node.xpath( './span[@class=&quot;content__list--item-price&quot;]/em/text()').get().strip() + '元/月' if item[&quot;address&quot;]: item[&quot;address&quot;] = item[&quot;address&quot;].strip() if item[&quot;big&quot;]: item[&quot;big&quot;] = item[&quot;big&quot;].strip() if item[&quot;where&quot;]: item[&quot;where&quot;] = item[&quot;where&quot;].strip() yield scrapy.Request( url=item[&quot;link&quot;], callback=self.detail_parse, meta={&quot;item&quot;: copy.deepcopy(item)}, dont_filter=True ) if self.page &lt; 24: next_url = 'https://jn.zu.ke.com/zufang/pg{}/#contentList'.format(self.page) self.page += 1 yield scrapy.Request(next_url, callback=self.parse) def detail_parse(self, response): item = response.meta['item'] item[&quot;name&quot;] = response.xpath('//*[@id=&quot;aside&quot;]/div[2]/div[2]/div[1]/span/text()').extract_first() yield item 运行命令进行爬取： 1scrapy crawl beike 使用pipline将数据保存到csv123456789101112131415import csvclass SavePipeline(object): def open_spider(self, spider): self.file = open(&quot;贝壳.csv&quot;, 'a', newline=&quot;&quot;,encoding=&quot;gb18030&quot;) self.csv_writer = csv.writer(self.file) self.csv_writer.writerow([&quot;标题&quot;, &quot;链接&quot;, '地址', &quot;大小&quot;, &quot;方向&quot;, &quot;居室&quot;, &quot;价格&quot;, &quot;名字&quot;]) def process_item(self, item, spider): self.csv_writer.writerow( [item[&quot;title&quot;], item[&quot;link&quot;], item[&quot;address&quot;], item[&quot;big&quot;], item[&quot;where&quot;], item[&quot;how&quot;], item[&quot;price&quot;], item[&quot;name&quot;]] ) return item def close_spider(self, spider): self.file.close() 实验总结Scrapy是用Python实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。Scrapy常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。我们可以很简单的通过Scrapy框架实现一个爬虫，抓取指定网站的内容或图片。","link":"/Article/Scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"},{"title":"Termux高级终端","text":"Termux 是一个 Android 终端仿真应用程序，用于在 Android 手机上搭建一个完整的 Linux 环境。 不需要 root 权限 Termux 就可以正常运行。Termux 基本实现 Linux 下的许多基本操作。可以使用 Termux 安装 python，并实现 python 编程，可以用手机架设 Server，同样可以用于渗透测试等等。 开发环境Termux 支持的开发环境很强，可以完美的运行 C、Python、Java、PHP、Ruby 等开发环境，建议朋友们选择自己需要的开发环境折腾。 编辑器写代码前总得折腾一下编辑器，毕竟磨刀不误砍柴工嘛。Termux 支持多种编辑器，完全可以满足日常使用需求。 VimVim 被称为编辑器之神，基本上 Linux 发行版都会自带 Vim，这个在前文基本工具已经安装了，如果你没有安装的话，可以使用如下命令安装： 1pkg install vim 并且官方也已经封装了 vim-python，对 Python 相关的优化。 1pkg install vim-python 解决汉字乱码如果你的 Vim 打开汉字出现乱码的话，那么在家目录 (~) 下，新建.vimrc 文件 1vim .vimrc 添加内容如下： 123set fileencodings=utf-8,gb2312,gb18030,gbk,ucs-bom,cp936,latin1set enc=utf8set fencs=utf8,gbk,gb2312,gb18030 然后 source 下变量： 1source .vimrc CTermux 官方封装了 Clang，他是一个 C、C++、Objective-C 和 Objective-C++ 编程语言的编译器前端。 安装 clang 1pkg install clang 编译测试clang 在编译这一块很强大，感兴趣的朋友可以去网上查看详细的教程，国光这里只演示基本的 Hello World 使用。写一个 Hello World 的 C 程序，如下 hello.c： 123456#include &lt;stdio.h&gt;int main(){ printf(&quot;Hello World&quot;) return 0;} 编辑完成后，使用 clang 来编译生成 hello 的可执行文件： 1clang hello.c -o hello JavaTermux 早期原生编译 JAVA 只能使用 ecj (Eclipse Compiler for Java) 和 dx 了，然后使用 Android 自带的 dalvikvm 运行。后面 Termux 官方也封装了 openjdk-17 这样安装起来就更方便了。 还有如果想要完整体验 JAVA 环境的话，另一个方法就是 Termux 里面安装一个完整的 Linux 系统，然后在 Linux 里面运行 Java，安装系统部分下面文章会详细介绍，这种思路也是可以的。 Openjdk-17 12pkg updatepkg install openjdk-17 当然这个包可能不太稳定，遇到相关问题可以去 Termux 官方的项目下提交 issue： https://github.com/termux/termux-packages/issues?q=openjdkhttps://github.com/termux/termux-packages/issues?q=java ECJ安装编译工具 1pkg install ecj dx -y 这里只演示基本的 Hello World 使用。写一个 Hello World 的 JAVA 程序，如下 HelloWorld.java: 12345public class HelloWorld { public static void main(String[] args){ System.out.println(&quot;Hello Termux&quot;); }} 编译生成 class 文件 1ecj HelloWorld.java 编译生成 dex 文件 1dx --dex --output=hello.dex HelloWorld.class 使用 dalvikvm 运行格式规范如下：dalvikvm -cp dex文件名 类名 1dalvikvm -cp hello.dex HelloWorld PythonPython 是近几年非常流行的语言，Python 相关的书籍和资料也如雨后春笋一般不断涌现，带来了活跃了 Python 学习氛围。 安装 Python2Python2 版本要淘汰了，大家简单了解一下就好： 1pkg install python2 -y 安装完成后，使用 python2 命令启动 Python2.7 的环境。 安装 Python3Termux 安装 Python 默认版本是 Python3 的版本，与此同时也顺便安装了 clang 1pkg install python -y 安装完成后，查看下 clang 和 Python 的版本： 1python -V 升级 pip21python2 -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple some-package 升级 pip31python -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple some-package 这两条命令分别升级了 pip2 和 pip3 到最新版。升级完成后你会惊讶的发现你的 pip3 命令不见了？？？不要慌 问题不大，我们可以手动查看当前有哪些可执行的 pip 文件，使用如下命令： 1ls /data/data/com.termux/files/usr/bin|grep pip 原来我们的pip3变成了pip3.8了啊 接下来分别查看对应 pip 可执行文件的版本：pip -V现在全都是最新版的 pip 了哦 Jupyter Notebook官方建议安装的完整的命令： 12345678910pkg updatepkg install proottermux-chrootapt install git clangapt install pkg-configapt install python python3-dev apt install libclang libclang-devapt install libzmq libzmq-devapt install ipython pip install jupyter 123456789101112131415161718# -i 手动指定国内中清华 pip 源 提高下载速度# 更新是个好习惯pkg update# 切换模拟的 root 环境termux-chroot# 安装相关依赖pkg install libclang# 安装 jupyterpip3 install jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple some-package# 安装完成退出 chrootexit# 安装 jupyterlabpip3 install jupyterlab -i https://pypi.tuna.tsinghua.edu.cn/simple some-package 安装好之后查看一下版本信息： 1jupyter --version 所有插件均安装完成 Jupyter Notebook 就安装好了，这个比较强大更详细的教程大家可以自行去谷歌或者百度一下，这里只演示基本的功能。 先启动 notebook 1jupyter notebook 然后会看到运行的日志，我们复制出 提示的 URL： 复制出的这个 URL 地址 在浏览器中打开： 可以看到成功运行了，那我们按照图片提示走个形式，输出个 Hello World 就跑路： OK 运行成功，那么回到 Termux 里面使用组合键 Ctrl + C -&gt; 中止当前的 Jupyter 进程。 Code-Server下载code-server的最新版本下载链接下载对应服务器版本的文件。 使用部署脚本一键安装 1curl -fsSL https://code-server.dev/install.sh | sh 运行code-server直接输入 code-server ，服务就自行开启了，默认是8080端口，127.0.0.1的访问ip。 如果想要自行更换ip或者是端口的话可以在后面添加–bind-addr参数。这里如果想让code-server被任意访问的话则需要把IP地址设置为0.0.0.0。bind-addr参数仅限code-server版本3.2.0及以上才会有，如果想知道具体参数可以输入code-server -h查看参数详情。 1code-server --bind-addr 0.0.0.0:[端口号] 安装完整Linux环境Termux 官网已经有官方版本的纯种Linux了，官方提供了最新的安装纯种Linux的方法：PRoot - Termux Wiki 首先最好是换个国内的Termux源，我用的清华源，换源方法看这里：Tsinghua Open Source Mirror 然后就是安装基础件proot-distro了： 1pkg install proot-distro 或者 1apt install proot-distro 查看proot-distro的使用帮助为： 1proot-distro help 查看可安装的系统列表 1proot-distro list 安装Ubuntu 1proot-distro install ubuntu 登录 1proot-distro login ubuntu 这样就进入了真正的linux环境了，以上就是官方版的纯种Linux安装全过程。","link":"/Article/Termux/"},{"title":"大数据技术原理与应用复习","text":"考试范围 ：教材:《大数据技术原理与应用》第三版 林子雨第1章：大数据概述 🎉第2章：大数据处理架构Hadoop 🎉第3章：分布式文件系统HDFS 🎉第4章：分布式数据库HBase🎉第5章：NoSQL数据库🎉第7章：MapReduce🎉第10章：Spark🎉第11章：流计算(Storm)🎉 第1章 大数据概述重要知识点：1、大数据的4V数据量大：在Web2.0时代、网络用户数目极大，在视频、照片社交软件火热的今天，用户产生的数据量非常大。数据类型繁多：有金融大数据、医疗大数据、城市大数据等等。处理速度快：1分钟，新浪微博产生2万条微博，Twitter产生10万条推文。价值密度低：例如监控视频的存储，没有调取监控视频时可能体现不出价值，但在使用时可能会产生很大的价值。 2、大数据的应用智能汽车、能源、体育与娱乐等等、大数据应用非常广泛。 3、大数据关键技术数据采集与预处理、数据存储和管理（HDFS）、数据处理与分析（如MapReduce）、数据安全和隐私保护。 4、大数据计算模式批处理计算：对大规模数据的批量处理 （MapReduce、Spark）流计算：针对流数据的实时计算(Spark)图计算：针对大规模图结构的数据的处理查询分析计算：大规模数据的存储管理和查询分析（Hive） 5、大数据与云计算、物联网的关系云计算为大数据提供技术基础、大数据为云计算提供用武之地。物联网是大数据的重要来源、大数据技术为物联网数据分析提供支撑。云计算为物联网提供海量数据存储能力，物联网为云计算技术提供了广阔的应用控件。 第2章 大数据处理架构Hadoop1、Hadoop的核心HDFS是分布式文件存储系统(Hadoop Distributed File System) 、也是Hadoop的核心。 2、Hadoop的特性高可靠性 、高效性、高可扩展性 、高容错性 、成本低 、支持操作系统与编程语言广泛 3、HDFS常用命令创建文件夹(-p 递归创建)：hadoop fs -mkdir -p /user/hadop显示文件内容：hadoop fs -cat hdfs文件文件上传：hadoop fs -put 本地 hdfs文件删除文件夹：hadoop fs -rm -r hdfs文件夹删除文件：hadoop fs -rm hdfs文件切换目录：hadoop fs -cd查看文件与目录：hadoop fs -ls -R(-R在hdfs为递归查看)删除空目录：hadoop fs -rmdir复制文件或目录：hadoop fs -cp移动：hadoop fs -mv创建文件：hadoop fs -touchz拉取文件：hadoop fs -get 第3章 分布式系统 HDFS1、 HDFS计算机集群结构分布式文件系统把文件分布存储到多个计算机节点上、成千上万的计算机节点构成计算机集群。 2、 HDFS的结构一个默认块大小为64MB，如果一个文件小于一个数据块的大小，在分布式文件系统，它并不占用整个数据块的存储空间。名称节点负责文件和目录的创建、删除、和重命名等，同时管理着数据节点和文件块的映射关系。数据节点负责数据的存储和读取，在存储时，由名称节点分配存储位置，客户端将数据写入相应数据节点。为了保证数据的完整性，文件块会被赋值为多个副本存储到多个不同的节点上，而存储同一文件块的不同副本又会分布在不同的机架上。 3、 HDFS特点优点：兼容廉价的设备硬件。流数据读写：支持流式方式来访问文件。大数据集：单个文件可达到GB甚至TB级别简单的文件模型：一次写入，多次读取。强大的跨平台兼容性。缺点：不适合低延迟数据访问。无法高效存储大量小文件。不支持多用户写入及任意修改文件。 4、 HDFS的相关概念（重点）重点：块、名称节点和数据节点与第二名称节点。块：HDFS文件块的大小默认为64MB，一个文件会被拆分为多个块进行存储。 (相关知识点:MapReduce中的Map任务一次只能处理一块中的数据。)采用块抽象化的好处 ：支持大规模文件存储、简化系统设计、适合数据备份。NameNode 负责管理分布式文件系统的命名空间，保存了FsImage和EditLog。DataNode 负责数据的存储和读取，根据客户端和名称节点的调度进行数据的存储和检索。也会定时向名称节点汇报自己的存储块列表信息。 FsImage 用于维护文件系统树以及文件树中所有的文件和文件夹的元数据。 EditLog (操作日志文件)记录了所有针对文件的创建、删除、重命名、等操作。 名称节点存储文件名与文件在数据节点中的位置的映射关系、但并不是持久化存储在名称节点，而是当名称节点启动时扫描分布式文件系统得到的。名称节点 启动将FsImage加载到内存-&gt;重新执行EditLog-&gt;建立新的空FsImage和EditLog-&gt;工作记录时使用新的FsImage和EditLog当变大后系统写入到总的FsImage和EditLog内（工作模式请看第二名称节点） 第二名称节点：功能：完成FsImage和EditLog的合并操作，减小EditLog文件大小。EditLog和FsImage的合并操作： 5、 HDFS体系结构主从结构客户端向名称节点请求文件名或者数据块号，名称节点将数据块号、数据块位置发送至客户端。客户端与数据节点通信进行数据的读写操作。HDFS集群只有一个名称节点，其带宽、计算性能会影响整个系统的性能。 HDFS命名空间管理HDFS集群只有一个命名空间,即/some/some/some/some… ,像Linux的文件目录结构一样。 通信协议HDFS通信协议建立在TCP/IP基础之上。名称节点与数据节点采用数据节点协议进行交互。客户端与数据节点交互通过远程过程调用(RPC) HDFS体系结构局限性命名空间限制、性能瓶颈(原因靠名称节点)、隔离问题（一个命名空间、和一个名称节点无法做到读写权限分配等）、集群可用性(一个名称节点) 6、 HDFS的存储原理多副本冗余存储，加快了数据传输的速率、容易检查数据错误、保证数据的可靠性。 存放策略HDFS默认的冗余复制因子是3，每一个文件块被保存到3个地方。（1）如果是集群内发起的写操作，则把第1个副本放置发起写操作请求的数据节点上，实现就近写入数据，如果集群外发起写操作，则在集群内选一个磁盘空间充足且CPU不太忙的数据节点，做为第1个副本的存放位置。（2） 第2个副本被放置在与第1个副本不同的机架的数据节点上。（3） 第3个副本会被放置在与第1个副本相同的机架的其他节点上。（4） 如果还有更多的副本、则继续从集群中随机选择数据节点进行存放。 读取策略客户端请求名称节点获取文件块不同副本的存储位置，由客户端确定从哪里获取文件块。 数据复制文件在客户端被切分成多个块，名称节点返回数据节点列表。客户端向一个数据节点写入，同时把数据节点列表传给第一个数据节点，当第一个节点接收数据大小4KB时并像列表第二个数据接待你发起写请求，将数据写入第二个数据节点、当第二个数据节点接收到4KB时向第三个数据节点执行类似操作。最后当文件写完时，数据的复制也同时完成了。 名称节点出错补救方案一：远程挂载到NFS上二：运行一个第二名称节点，能够进行有限的补救(第二名称节点中由FsImage和EditLog),但还是会可能遗失拉去FsImage和EditLog后，名称节点的一系列操作。数据节点出错补救方案数据节点定期向名称节点发起”心跳“，没有”心跳”时，名称节点不再向其分配读写任务，且一旦发现某些块的复制因子小了，名称节点又会安排任务，进行文件块复制。数据出错客户端采用MD5和SHA-1校验，出错将向名称节点汇报，请求其他副本，名称节点也会定期检查块。 第4章 分布式数据库HBase1、HBase是什么?一个高可靠、高性能、面向列、可伸缩的分布式数据库。2、HBase与Hadoop其他部分关系。使用MapReduce处理海量数据，ZooKeeper作为协同服务。HDFS底层数据存储。Pig和Hive提供高级语言支持。3、HBase与传统数据库对比。数据类型：采用未经解释的字符串。数据操作：行键查询、只有简单的插入、查询、删除、清空等。存储模式：列式存储。数据索引：行键索引。数据维护：并不删除原来的数据。横向可伸缩性：面向列存储。缺陷：不支持事务、无法实现跨行原子性。4、HBase数据模型行键：任意字符串，最大64KB。列族、列限定符(列名，不需要提前定义好)。单元格，每个单元格可以存储多个版本、每个版本对应一个时间戳。事件戳：每次对单元格执行操作，HBase会隐式自动生成并存储一个时间戳。数据坐标：[行键、列族、列限定符、时间戳]5、概念视图与物理视图概念视图：逻辑模型，认为规定建立。 物理视图： 在物理视图中，空的列并不会被存储为null，而是不会存储，当请求空白的单元格时返回null。 6、 面向列的存储从严格的关系数据库角度来看，HBase并不是一个列式存储的数据库，HBase是以列族为单位进行分解的，而不是每个列都单独存储。 7、 HBase实现原理与HDFS具有很多相似之处、有一个Master节点负责HBase表的分区信息，表被分为Region存储到多个Region服务器，Master会检测Region服务器工作状态、负责性能均衡。客户端并不从Master获得数据，从ZooKeeper获得Region位置，从Region服务器中拉取数据。 8、 表和Region、Region的定位 9、 HBase系统架构库函数：链接到每个客户端客户端：缓存Region位置、与Master进行RPC通信、与Region服务器RPC通信进行数据的读写。Zookeeper服务器：通常由集群组成，每个Region服务器要到Zookeeper服务器注册，Zookeeper实时监控Region,Master通过Zookeeper感知Region服务器状态。Master服务器：管理用户对表操作、Region服务器的负载均衡、Region分裂与合并、Region迁移Region服务器：Region存储、向客户端提供访问接口。 10、 Region服务器工作原理 第5章 NoSQL数据库1、 NoSQL特点灵活的可扩展性、灵活的数据模型、与云计算紧密耦合。 2、 NoSQL与Web2.0无法满足海量数据的管理需求、无法满足高并发的需求、无法满足高可扩展性和高可复用性的要求。Web2.0网站系统通常不需要严格的数据库事务、并不要求严格的读写实时性、通常不包含大量复杂的SQL查询。 3、 NoSQL的四大类型键值数据库、列族数据库、文档数据库、图数据库。 4、 NoSQL三大基石，CAP、BASE、最终一致性。C (Consistency) :一致性 ，指任何一个读操作总是能够读到之前完成的写操作的结果，在分布式环境中、多点数据是一致的。A（Availability）：可用性 。指快速获取数据，且可以在确定的时间内返回操作结果。P（tolerance of network partition）：分区容忍性 ，当出现网络分区的情况，分离的系统可以正常运行。 组合：CA 强调一致性和可用性，可扩展性较差。CP 强调一致性和分区容忍性，当出现网络分区的情况时，受影响的服务需要的等待数据一致，因此在等待期间就无法对外提供服务。AP 在采用AP设计时，可以不完全放弃一致性，转而采用最终一致性。 BASEA(atomicity)原子性 ：对数据的修改、要么全部执行、要么全部不执行。C(Consistency)一致性 ：在事务完成时，必须是所有数据都保持一致状态。I(Isolation) 隔离性 ：并发事务所作的修改必须与其他并发事务所做的改变隔离。D(Durability) 持久性 ：事务完成后、对系统的影响是永久性的。 最终一致性是弱一致性的特例，允许后续的访问操作可以暂时读不到更新后的数据，但经过一段时间后，用户可以读到更新后的数据，最终一致性也是ACID的最终目的，只要最终数据是一致的就可以了，而不是每时每刻都保持实时一致性。 第7章 MapReduceHadoop MapReduce是基于谷歌MapReduce的开源实现。 大规模数据集 -&gt; 分片小数据集 -&gt; 处理为 -&gt; 交给Map任务 -&gt; 输出List() -&gt; Map Shuffle -&gt; ReduceShuffle -&gt; 输入 -&gt; Reduce -&gt; 输出。 当Map任务全部结束时，才会开始Reduce任务。在切分大文件时没并不是真正的切分物理文件、而是利用RecordReader记录要处理数据的位置和长度。 Shuffle过程，分为Map端的Shuffle过程、和Reduce端的Shuffle过程，Shuffle是指对Map任务输出结果进行分区，排序、合并、归并等处理并交给Reduce的过程。 Map端的Shuffle过程输入数据和执行Map任务，Map输出结果写入缓存、缓存满了则进行溢写(分区、排序、合并)生成多个溢写文件、当Map任务全部结束之前，溢写文件会被归并为一个大的磁盘文件。 Reduce任务从Map端的不同Map机器”领取”属于自己处理的那部分数据，然后对数据进行归并后交给Reduce处理。具有相同key的会被发送到同一个Reduce任务。 关于 溢写，提供给MapReduce的缓存的容量是有限的，默认大小100MB。随着Map任务执行，很快写满缓存区，进行溢写操作，首先对这些键值进行分区，默认的分区方式为Hash函数对key哈希。“合并”是指将具有相同key的的value加起来。并非所有场景都适合合并操作。每次溢写操作都会生成一个新得溢写文件，写入溢写文件中得所有键值对都是经过分区和排序得。 “归并” ,溢写文件数量越来越多，最终在Map任务全部结束前，系统对所有溢写文件数据进行归并操作，具有相同key的键值被合并为一个键值，如会被归并为一个新的键值，&gt;。JobTracker检测Map任务，当Map任务完成时，通知Reduce任务来领取数据。然后开始Reduce端的Shuffle过程。 Reduce端的Shuffle过程从Map端读取Map任务结果，执行归并操作，，最后送给Reduce任务进行处理。JobTracker检测Map任务，通知Reduce任务领取数据，先放置到Reduce任务机器的缓存中，同样存在溢写操作，当溢写过程启动时，具有相同key的键值对会被归并。同样存在溢写文件的合并。经过多轮归并后得到若干个大文件，直接输入Reduce任务。 WordCount实例 MapReduce模型的关系上的标准运算选择、投影、并、交、差、自然连接 第10章 Spark1、 什么是Spark?Spark是基于内存计算的大数据并行计算框架，可用于构建大型的、低延时的数据分析应用程序.2、 Spark的特点。运行速度快、使用有向无环图执行引擎、支持循环数据流与内存计算。容易使用，支持多种编程语言。通用性：提供完整的技术栈，SQL查询、流式计算、机器学习等等。运行模式多样：可运行在集群、EC2等环境中。 3、 Spark与Hadoop的对比 Hadoop缺点 ：（1）表达能力有限，需要转化为Map和Reduce操作。（2）磁盘IO开销大，需要先内存缓存写入，缓存内容溢写到磁盘。（3）延迟高：一次计算可能要多个MapReduce任务，任务之间设计IO开销，在前一个任务完成之前，其他任务无法开始。 Spark优点：（1） Spark计算模式也属于MapReduce，但不局限于Map和Reduce操作。（2） 提供内存计算、计算结果放置于内存中。（3） 基于DAG任务调度执行机制，由于MapReduce的迭代执行机制。 4、 Spark生态系统大数据处理主要包括3个·类型（1） 复杂的批量数据处理（2） 基于历史数据的交互式查询（3） 基于实时数据流的数据处理 Spark应用场景场景 时间跨度 spark生态组件复杂的批量数据处理 小时级 spark core基于历史数据的交互式查询 分钟、秒级别 Spark SQL实时数据流的数据处理 毫秒级、秒级别 spark streaming、structured streaming历史数据的数据挖掘 MLib图结构数据的处理 GraphX 5、 Spark运行架构RDD：一种高度受限的共享内存模型DAG：反应RDD之间的依赖关系Spark基本运行流程： 6、 RDD相关原理 7、 RDD特性（1） 高效的容错性（2） 中间结果持久化道内存（3） 存放的数据可以是Java对象 8、 RDD之间的依赖关系 M*R个bucket M: map任务数量 R：Reduce任务数量Spark多个桶写入同一个文件Map任务产生 数据文件与索引文件，Reduce任务通过索引文件信息，获取自己应该处理的数据信息。Reduce任务并不进行排序，而是利用HashMap进行分类，Reduce内存必须满足存放所有其应该存储的否则内存会溢出。但内存过大时操作也会从内存到磁盘。Spark的Shuffle过程也有把数据写入到磁盘的情况。 9、 宽依赖与窄依赖 10、 阶段划分只有窄依赖才能完成流水线优化。Spark通过分析RDD之间的依赖关系生成DAG，再通过分析各个RDD中的分区之间的依赖关系，划分阶段。具体方法：在DAG中反向解析，遇到宽依赖就断开，遇到窄依赖就把当前的RDD加入当前的阶段。 11、 RDD运行过程（1） 创建RDD对象（2） SparkContent负责计算RDD之间的依赖关系，构建DAG（3） DAGScheduler负责把DAG分解成多个阶段、每个阶段中包含多个任务，每个任务会被任务调度器分发给各个工作节点上的Executor去执行 第11章 流计算1、 流数据（1） 快速持续到达、潜在数据量也许是无穷无尽的（2） 数据来源众多、格式复杂（3） 数据量大、但不是十分关注存储。（4） 注重数据的整体价值，不过分关注个别数据（5） 数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序。 2、 流计算MapReduce负责海量数据执行批量计算（1） 高性能 （2）海量式 （3）实时性 （4）分布式（5）易用性 （6）可靠性 3、 传统数据处理与流计算对比 4、 数据处理流程详情（1） 数据实时采集Agent：主动采集数据、并把数据推送到Collector部分Collector：接收多个Agent的数据，并实现有序、可靠、高性能转发Store：存储，一般不存储，直接发送给流计算平台进行计算 （2） 数据实时计算数据流入 流处理系统实时计算 计算结果与数据流出 （3） 实时查询服务流计算处理的数据是实时的，用户通过流处理系统获取的是实施结果，无须人为查询，系统推送给用户 5、 流计算应用场景（1） 实时分析，如用户商品推荐、广告推荐。（2） 实时交通，导航路线实时交通状况。 6、 StormStorm 免费的、开源的分布式实时计算系统。 7、 Storm的特点（1） 整合性。方便地与队列系统和数据库系统进行整合（2） 简易的API。Storm的API在使用上简单方便。（3） 可扩展性。并行特点，可以部署在分布式集群。（4） 容错性。自动故障节点重启。（5） 可靠的消息处理。保证每个消息都能完整处理。（6） 支持各种编程语言。（7） 快速部署。（8） 开源、免费。 8、 Storm设计思想Streams:流数据是一个无限的Tuple序列。Spouts：流数据的源头。Bolts：流数据的转换过程。Topology：Spouts和Bolts组成的拓扑网络，流转换图。Stream Groupings: 告知两个Bolt之间怎样进行Tuple传递。ShuffleGrouping: 随机分组，随即分发Stream中的Tuple,保证每个Bolt的Task接收Tuple数量大致一致。FiledsGouping: 按照字段分组，保证相同字段的Tuple分配到同一个Task中。AllGrouping: 广播发送，每一个Task都会收到所有的Tuple.GlobalGrouping：全局分组，所有Tuple分到同一个Task中。NonGrouping： 不分组。DirectGrouping：直接分组，直接指定由某个Task来执行Tuple的处理。 9、 Storm的框架设计 （1） 提交Topology到Storm集群（2） Nimbus将分配给Supervisor的任务写入Zookeeper（3） Supervisor从Zookeeper获取所分配的任务，并启动Worker进程。（4） Worker进行执行任务。 10、 Spark StreamingSpark Streaming提供 整合多种输入数据源，将实时输入数据流以时间片为单位进行拆分，经过Spark引擎处理每个时间片数据，SparkStreaming的输入数据按照时间片分成一段一段的Dstream,每一段数据转换为Spark中的RDD,对Dstream的操作最终转变为相对应的RDD操作。Spark streaming无法完成毫秒级的流计算，因为其将流数据按批处理窗口大小分解为一系列的批量处理的作业。 HBase案例题客户端提供访问HBase的接口，Zookeeper服务器保存-ROOT-表的地址和Master主服务器的地址，通过三层寻址找到所需的数据。Master主服务器主要负责表和Region的管理工作，Region服务器负责维护分配给自己的Region，并响应用户请求，HBase采用HDFS作为底层存储的文件系统，HDFS向HBase提供可靠的数据存储。 1.试述在Hadoop体系架构中HBase与其他组成部分的相互关系​答： HBase利用Hadoop MapReduce来处理HBase中的海量数据，实现高性能计算；利用Zookeeper作为协同服务，实现稳定服务和失败恢复；使用HDFS作为高可靠的底层存储，利用廉价集群提供海量数据存储能力; Sqoop为HBase的底层数据导入功能，Pig和Hive为HBase提供了高层语言支持，HBase是BigTable的开源实现。 3.请阐述HBase和传统关系数据库的区别 区别 传统关系数据库 HBase 数据类型 关系模型 数据模型 存储模式 基于行模式存储，元组或行会被连续地存储在磁盘也中 基于列存储，每个列族都由几个文件保存，不同列族的文件是分离的 数据索引 针对不同列构建复杂的多个索引 只有一个行键索引 数据维护 用最新的当前值去替换记录中原 来的旧值 更新操作不会删除数据旧的版本，而是生成一个新的版本 可伸缩性 很难实现横向扩展，纵向扩展的空间也比较有限 轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩 4.HBase有哪些类型的访问接口？答：HBase提供了Native Java API , HBase Shell , Thrift Gateway , REST GateWay , Pig , Hive 等访问接口。 5.请以实例说明HBase数据模型。 6.分别解释HBase中行键、列键和时间戳的概念行键是唯一的，在一个表里只出现一次，否则就是在更新同一行，行键可以是任意的字节数组。列族需要在创建表的时候就定义好。列族名必须由可打印字符组成，创建表的时候不需要定义好列。时间戳，默认由系统指定，用户也可以显示设置。使用不同的时间戳来区分不同的版本。 7.请举个实例来阐述HBase的概念视图和物理视图的不同HBase数据概念视图 HBase数据物理视图在HBase的概念视图中，一个表可以视为一个稀疏、多维的映射关系。在物理视图中，一个表会按照属于同一列族的数据保存在一起。 8.试述HBase各功能组建及其作用。（1）库函数：链接到每个客户端；（2）一个Master主服务器：主服务器Master主要负责表和Region的管理工作；（3）许多个Region服务器：Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求 9.请阐述HBase的数据分区机制。答： HBase采用分区存储，一个大的表会被分拆许多个Region，这些Region会被分发到不同的服务器上实现分布式存储。 10.HBase中的分区是如何定位的。答： 通过构建的映射表的每个条目包含两项内容，一个是Regionde 标识符，另一个是Region服务器标识，这个条目就标识Region和Region服务器之间的对应关系，从而就可以知道某个Region被保存在哪个Region服务器中。 11.试述HBase的三层结构中各层次的名称和作用。 12.请阐述HBase的三层结构下，客户端是如何访问到数据的。答：首先访问Zookeeper，获取-ROOT表的位置信息，然后访问-Root-表，获得.MATA.表的信息，接着访问.MATA.表，找到所需的Region具体位于哪个Region服务器，最后才会到该Region服务器读取数据。 13.试述HBase系统基本架构以及每个组成部分的作用。（1）客户端客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程 （2）Zookeeper服务器Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题 （3）Master主服务器Master主要负责表和Region的管理工作：管理用户对表的增加、删除、修改、查询等操作；实现不同Region服务器之间的负载均衡；在Region分裂或合并后，负责重新调整Region的分布；对发生故障失效的Region服务器上的Region进行迁移 （4）Region服务器Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求 14.请阐述Region服务器向HDFS文件系统中读写数据的基本原理。​Region服务器内部管理一系列Region对象和一个HLog文件，其中，HLog是磁盘上面的记录文件，它记录着所有的更新操作。每个Region对象又是由多个Store组成的，每个Store对象了表中的一个列族的存储。每个Store又包含了MemStore和若干个StoreFile，其中，MemStore是在内存中的缓存。 15.试述HStore的工作原理每个Store对应了表中的一个列族的存储。每个Store包括一个MenStore缓存和若干个StoreFile文件。MenStore是排序的内存缓冲区，当用户写入数据时，系统首先把数据放入MenStore缓存，当MemStore缓存满时，就会刷新到磁盘中的一个StoreFile文件中，当单个StoreFile文件大小超过一定阈值时，就会触发文件分裂操作。 16.试述HLog的工作原理答：HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log），用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘。","link":"/Article/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%A4%8D%E4%B9%A0/"},{"title":"机器学习复习","text":"1、k近邻算法：使用sklearn中的相关函数实现鸢尾花案例的k近邻分类，尝试使用不同的k值以比较实验结果。最后，请用以下函数实现K值自动调优。（交叉验证，网格搜索（模型选择与调优）API：sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None) 2、决策树算法：如图数据集，住房（1表示拥有住房，0表示没有住房）；婚姻（0表示单身，1表示已婚，2表示离异）；年收入一栏中单位为1000元；（拖欠贷款一栏0表示不拖欠，1表示拖欠） 任务：使用sklearn的tree函数、 graphviz函数，选择合适的算法，构造并绘制决策树 HOUSE Marriage Annual salary No default/Default 1 0 125 0 0 1 100 0 0 0 70 0 1 1 120 0 0 2 95 1 0 1 60 0 1 2 220 0 0 0 85 1 0 1 75 0 0 0 90 1 k近邻算法 利用sklearn中自带的鸢尾花数据集，并通过KNN算法实现对鸢尾花的分类，使用不同的k值比较实验结果，最后使用以下函数实现K值自动调优。 1234sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)estimator：估计器对象param_grid：估计器参数(dict){“n_neighbors”:[1,3,5,10]}cv：指定几折交叉验证 关键代码 123456789101112131415161718192021222324252627282930# 1.获取鸢尾花的特征值，目标值iris_data, iris_target = self.get_iris_data()# 2.将数据分割成训练集和测试集 test_size=0.30表示将30%的数据用作测试集 random_state=0可以将数据打散后再分割x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_target, random_state=0) # 将数据随机打散后可以看到准确率可以达到 97%x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_target, test_size=0.30)# 3.特征工程(对特征值进行标准化处理)std = StandardScaler()x_train = std.fit_transform(x_train)x_test = std.transform(x_test) # 4.送入算法print(&quot;使用不同k制进行预测&quot;)for k in range(1, 11): knn = KNeighborsClassifier(n_neighbors=k) knn.fit(x_train, y_train) print(k, &quot;准确率：&quot;, knn.score(x_test, y_test))# 5.使用GridSearchCV函数实现K值自动调优# 生成knn估计器knn = KNeighborsClassifier()# 构造超参数值params = {&quot;n_neighbors&quot;: [3, 5, 10]}# 进行网格搜索gridCv = GridSearchCV(knn, param_grid=params, cv=25)gridCv.fit(x_train, y_train) # 输入训练数据# 预测准确率print(&quot;准确率：&quot;, gridCv.score(x_test, y_test))print(&quot;交叉验证中最好的结果：&quot;, gridCv.best_score_)print(&quot;最好的模型：&quot;, gridCv.best_estimator_) 运行截图 决策树算法 首先要选择最优的划分属性，尽量使划分的样本属于同一类别，即“纯度”最高的属性。定义数据的“纯度”就成了问题的关键，主流的算法是CART算法，sklearn中DecisionTreeClassifier就是用的CART算法，其判别纯度的指标是entropy熵与Gini指数。 熵的公式为： K表示有K种类别，pi是X取第i种类的概率，实际计算直接用第i种类别所占的比例代替，熵越大，表示此时的混乱程度越大，数据越不纯。 如果利用某一种指标A=a将原数据D分为两类D1与D2之后，在这个分类下，定义此时的条件熵为： 定义此时的基尼指数为： CART算法生成的决策树是二叉树，每一步只对某一个指标做出划分。如果特征是离散的取值，那么就对每个特征的每个不同的取值作为二叉树的判定标准，大于或者小于等于该值分成两类，并计算以该点分类后，新节点的信息增益或者基尼指数，以两个子节点的样本数占比进行加权计算；如果特征是连续的取值，那么以每两个相邻的特征取值的算术平均离散化。 设需要分类的数据为D，定义数据集内数据量为D，属性集S，算法的伪代码如下： ① 设定纯度判断阈值与样本数量阈值等防止树过深的参数 ② 若)或或没有特征，则停止，返回当前树，否则转③ ③根据所有特征的值进行二分类，选出基尼指数最小或信息增益最大的那个特征的值作为分类依据，得到的两个子数据集D1,D2 并返回② 关键代码 12345678910111213141516171819tree_model = tree.DecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_leaf=1, ccp_alpha=0.0)tree_model.fit(X, y)dot_data = StringIO()feature_names = ['House', 'Marriage', 'Annual salary']target_names = ['No default', 'Default']tree.export_graphviz(tree_model, out_file=dot_data, feature_names=feature_names, class_names=target_names, filled=True, rounded=True, special_characters=True)dot_data = dot_data.getvalue()dot_data = dot_data.replace('\\n', '')graph = pydotplus.graph_from_dot_data(dot_data.getvalue())graph.write_pdf(&quot;default.pdf&quot;) 20Newsgroups文档分类 基于20Newsgroups数据集，利用朴素贝叶斯算法（请尝试使用不同的分类器，如GaussianNB、MultinominalNB等），对数据集进行文本分类，结合交叉验证，对分类结果进行简要阐释。 下载数据集 1news = fetch_20newsgroups(subset='all') 文本数据属于非结构化的数据，一般要转换成结构化的数据才能够通过机器学习算法进行分类。常见的做法是将文本转换成“文档-词项矩阵”，矩阵中的元素可以使用词频或TF-IDF值等。 首先分割训练数据和测试数据，然后采用普通统计CountVectorizer提取特征向量，然后再采用TfidfVectorizer提取文本特征向量，然后实例化一个朴素贝叶斯分类器，并将我们的训练集特征与训练集特征对应的分类结果导入到模型中并训练模型，传入测试集，进行预测，并打印预测结果与预测精度。 关键代码 123456789101112131415161718192021222324252627282930313233343536373839# 分割训练数据和测试数据x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=0)# 采用普通统计CountVectorizer提取特征向量count_vec = CountVectorizer()x_count_train = count_vec.fit_transform(x_train)x_count_test = count_vec.transform(x_test) # 采用TfidfVectorizer提取文本特征向量tfid_vec = TfidfVectorizer()x_tfid_train = tfid_vec.fit_transform(x_train)x_tfid_test = tfid_vec.transform(x_test)mnb_count = MultinomialNB()mnb_count.fit(x_count_train, y_train)mnb_count_y_predict = mnb_count.predict(x_count_test)print(&quot;测试集的预测结果为：&quot;, mnb_count_y_predict)print(&quot;准确率：&quot;, mnb_count.score(x_count_test, y_test))print(&quot;详细的评估指标:\\n&quot;, classification_report(mnb_count_y_predict, y_test))mnb_tfid = MultinomialNB()mnb_tfid.fit(x_tfid_train, y_train)mnb_tfid_y_predict = mnb_tfid.predict(x_tfid_test)print(&quot;测试集的预测结果为：&quot;, mnb_tfid_y_predict)print(&quot;准确率：&quot;, mnb_tfid.score(x_tfid_test, y_test))print(&quot;详细的评估指标:\\n&quot;, classification_report(mnb_tfid_y_predict, y_test))# 设置评估算法的基准num_folds = 10seed = 7scoring = 'accuracy'# 交叉验证 自动调参param_grid = {'alpha': [0.001, 0.01, 0.1, 1.5]}model = MultinomialNB()kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)grid_result = grid.fit(X=x_tfid_train, y=y_train)print('最优：%s 使用%s' % (grid_result.best_score_, grid_result.best_params_)) 运行截图 逻辑回归与梯度下降 1、 根据小批量梯度下降MBGD的原理，写出python函数代码。 2、 使用MBGD实现患疝气病的马存活问题的Logistic回归来预测 改变batch_size的大小，比较模型表现，选择最优batch_size将预测结果与BGD、SGD的结果进行比较与讨论（准确度、收敛速度等） 小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用batch_size个样本来对参数进行更新。假设 batchsize=10，样本数 m=1000。伪代码形式为： 关键代码 12345678910111213141516171819202122232425262728293031def MBGD(dataSet, alpha=0.001, batch_size=10, sample=368): xMat = np.mat(dataSet.iloc[:, :-1].values) yMat = np.mat(dataSet.iloc[:, -1].values).T xMat = regularize(xMat) m, n = xMat.shape weights = np.zeros((n, 1)) m = int(sample / batch_size) print(&quot;batch_size =&quot;, batch_size, end=&quot; &quot;) for i in range(m): for i in range(batch_size): grad = xMat.T * (xMat * weights - yMat) / batch_size weights = weights - alpha * grad return weightsdef get_acc(train, test, alpha=0.001, maxCycles=5000, batch_size=10, sample=1000): # weights = SGD_LR(train, alpha=alpha, maxCycles=maxCycles) weights = MBGD(train, alpha=alpha, batch_size=batch_size, sample=sample) xMat = np.mat(test.iloc[:, :-1].values) xMat = regularize(xMat) result = [] for inX in xMat: label = classify(inX, weights) result.append(label) retest = test.copy() retest['predict'] = result acc = (retest.iloc[:, -1] == retest.iloc[:, -2]).mean() print(f'模型准确率为：{acc}') return retestfor i in range(1, 35): get_acc(train, test, alpha=0.001, batch_size=i) 由结果可知，最优batch_size为21 对比BGD与SGD：BGD每一次迭代时使用所有样本来进行梯度的更新，SGD是每次迭代使用一个样本来对参数进行更新，对于BGD而言，每次迭代需要计算所有样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设10次）。对于SGD，每次更新参数只需要一个样本。假设样本有30W个，若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。在收敛时，BGD计算了10×30W次，而SGD只计算了1×30W次。从迭代的次数上来看，SGD迭代的次数较少，在解空间的搜索过程看起来很盲目。而MBGD使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。（设置batch_size=100时，只需要迭代3000次） 总结Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。MBGD通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多，每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果，且可实现并行化。 支持向量机 1、 使用Iris原始数据集，构建多分类SVM模型 尝试改变各种超参数，特别是多分类参数decision_function_shape，比较模型结果是否存在差异输出各类的支持向量的个数自主探究对于多分类问题，如何可视化分类结果 2、 手写数字分类 比较使用决策树、朴素贝叶斯及支持向量机同样实现手写数字分类的效果的不同，简述三种方法各自的优劣 sklearn.svm.SVC 支持向量机参数 123456789class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)• C：正则化参数。正则化的强度与C成反比。必须严格为正。惩罚是平方的l2惩罚。(默认1.0)， 惩罚参数越小，容忍性就越大• kernel：核函数类型，可选‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’• degree：当选择核函数为poly多项式时，表示多项式的阶数。• gamma：可选‘scale’和‘auto’，表示为“ rbf”，“ poly”和“ Sigmoid”的内核系数。默认是'scale',gamma取值为1 / (n_features * X.var())；当选‘auto’参数时gamma取值为1 / n_features。• decision_function_shape：多分类的形式，1 vs 多(‘ovo’)还是1 vs 1(’ovr’)，默认’ovr’• probability：是否启用概率估计,默认是False。必须在调用fit之前启用此功能，因为该方法内部使用5倍交叉验证，因而会减慢该方法的速度，并且predict_proba可能与dict不一致。• max_iter：算法迭代的最大步数，默认-1表示无限制• random_state：随机种子，随机打乱样本。 完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from matplotlib import pyplot as pltfrom sklearn import datasetsfrom sklearn import svmfrom sklearn.model_selection import train_test_splitimport numpy as np# 加载数据集iris = datasets.load_iris()X = iris.data[:, :2] # 取前两维特征y = iris.target# 画出数据的散点图来看看分布状况plt.scatter(X[y == 0, 0], X[y == 0, 1], color='r', marker='o')plt.scatter(X[y == 1, 0], X[y == 1, 1], color='b', marker='*')plt.scatter(X[y == 2, 0], X[y == 2, 1], color='g', marker='+')plt.xlabel('x1')plt.ylabel('y1')plt.show()# 模型的训练X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0)# C是误差惩罚系数，用来调节模型方差与偏差的问题C = 1.0lin_svc = svm.SVC(decision_function_shape='ovo', kernel='linear', C=C).fit(X_train, y_train) # 线性核rbf_svc = svm.SVC(decision_function_shape='ovo', kernel='rbf', gamma='auto', C=C).fit(X_train, y_train) # 径向基核poly_svc = svm.SVC(decision_function_shape='ovo', kernel='poly', degree=3, C=C).fit(X_train, y_train) # 多项式核print(&quot;支持向量个数：&quot;,len(lin_svc.support_vectors_))print(&quot;支持向量个数：&quot;,len(rbf_svc.support_vectors_))print(&quot;支持向量个数：&quot;,len(poly_svc.support_vectors_))# 模型的评估h = .02 # 网格中的步长x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))titles = ['LinearSVC (linear kernel)', 'SVC with RBF kernel', 'SVC with polynomial (degree 3) kernel']for i, clf in enumerate((lin_svc, rbf_svc, poly_svc)): # 绘出决策边界，不同的区域分配不同的颜色 plt.subplot(2, 2, i + 1) # 创建一个2行2列的图，并以第i个图为当前图 plt.subplots_adjust(wspace=0.4, hspace=0.4) # 设置子图间隔 # 将xx和yy中的元素组成一对对坐标，作为支持向量机的输入，返回一个array Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # 把分类结果绘制出来 Z = Z.reshape(xx.shape) # (220, 280) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8) # 使用等高线的函数将不同的区域绘制出来 plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired) # cmap相当于是配色盘 # 将训练数据以离散点的形式绘制出来 plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.xticks(()) plt.yticks(()) plt.title(titles[i])plt.show()print(&quot;linear&quot;)print(lin_svc.predict(X_test))print(&quot;score:&quot;, lin_svc.score(X_test, y_test))print(&quot;rbf&quot;)print(rbf_svc.predict(X_test))print(&quot;score:&quot;, rbf_svc.score(X_test, y_test))print(&quot;poly&quot;)print(poly_svc.predict(X_test))print(&quot;score:&quot;, poly_svc.score(X_test, y_test)) 不同参数的影响1. 参数C和gamma都会影响支持向量的数量。参数C越大，支持向量的数量越少。2. 当 gamma 很小时，每一个支持向量的影响范围很大，甚至可以包含整个训练集，从而使得模型受限于每一个数据点，并不能很好的反应数据的复杂性。这样结果就是模型会接近线性。当gamma 很大时，每一个支持向量的影响范围很小，导致C并不能很好地实现 regularization 从而无法避免过拟合。3. 仅凭支持向量的数量，不能很好地说明模型是否存在过拟合（或者欠拟合）的问题。4. 更少的支持向量占用的存储空间更小，预测速度更快。 支持向量的个数 手写数字分类 关键代码 123456mnist = load_digits()x, test_x, y, test_y = train_test_split(mnist.data, mnist.target, test_size=0.25, random_state=40)model = svm.LinearSVC(max_iter=10000)model.fit(x, y)z = model.predict(test_x) 对比三种算法输出的结果1. SVM优点：适合小样本、非线性、高维模式识别。缺点：对于大规模数据开销大，不合适多分类；对缺失数据敏感；需要选择适当的核函数。2. 决策树优点：简单易于理解，能够处理多路输出问题。缺点：容易过拟合；决策树的生成不稳定，微小的数据变化可能导致生成的决策树不同。3. KNN优点：简单易于理解，无需训练，无需估计参数准确性高；适合多标签问题。缺点：懒惰算法，预测慢，开销大类的样本数不平衡时准确率受影响；可解释性差。 运行截图 总结SVM的主要思想是：建立一个超平面作为决策平面，使得正例和反例之间的隔离边缘被最大化。SVM也是结构风险最小化方法的近似实现。SVM是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机。由于在线性支持向量机学习的对偶问题里，目标函数和分类决策函数都只涉及实例和实例之间的内积，所以不需要显式地指定非线性变换，而是用核函数替换当中的内积。","link":"/Article/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"title":"迁移博客到Hexo","text":"Hexo简介Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub上，是搭建博客的首选框架。可以进入hexo官网进行详细查看。 教程分三个部分： 第一部分：hexo的初级搭建还有部署到github page上，以及个人域名的绑定。 第二部分：hexo的基本配置，更换主题，实现多终端工作，以及在coding page部署实现国内外分流 第三部分：hexo添加各种功能，包括搜索的SEO，阅读量统计，访问量统计和评论系统等。 第一部分hexo的初级搭建还有部署到github page上，以及个人域名的绑定。 Hexo搭建步骤 安装Git 安装Node.js 安装Hexo GitHub创建个人仓库 生成SSH添加到GitHub 将hexo部署到GitHub 设置个人域名 发布文章 1. 安装GitGit是目前世界上最先进的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。也就是用来管理你的hexo博客文章，上传到GitHub的工具。Git非常强大，我觉得建议每个人都去了解一下。廖雪峰老师的Git教程写的非常好，大家可以了解一下。Git教程 windows：到git官网上下载,Download git,下载后会有一个GitBash的命令行工具，以后就用这个工具来使用git。 linux：对linux来说实在是太简单了，因为最早的git就是在linux上编写的，只需要一行代码 1sudo apt-get install git 安装好后，用git --version 来查看一下版本 2. 安装nodejsHexo是基于nodeJS编写的，所以需要安装一下nodeJs和里面的npm工具。 windows：nodejs选择LTS版本就行了。 linux： 12sudo apt-get install nodejssudo apt-get install npm 安装完后，打开命令行 12node -vnpm -v 检查一下有没有安装成功 顺便说一下，windows在git安装完后，就可以直接使用git bash来敲命令行了，不用自带的cmd，cmd有点难用。 3. 安装hexo前面git和nodejs安装好后，就可以安装hexo了，你可以先创建一个文件夹blog，然后cd到这个文件夹下（或者在这个文件夹下直接右键gitbash打开）。 输入命令 1npm install -g hexo-cli 依旧用hexo -v查看一下版本 至此就全部安装完了。 接下来初始化一下hexo 1hexo init myblog 这个myblog可以自己取什么名字都行，然后 12cd myblog //进入这个myblog文件夹npm install 新建完成后，指定文件夹目录下有： node_modules: 依赖包 public：存放生成的页面 scaffolds：生成文章的一些模板 source：用来存放你的文章 themes：主题 ** _config.yml: 博客的配置文件** 12hexo ghexo server 打开hexo的服务，在浏览器输入localhost:4000就可以看到你生成的博客了。 大概长这样：使用ctrl+c可以把服务关掉。 4. GitHub创建个人仓库首先，你先要有一个GitHub账户，去注册一个吧。 注册完登录后，在GitHub.com中看到一个New repository，新建仓库 创建一个和你用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHubpage的时候，才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名。我这里是已经建过了。 点击create repository。 5. 生成SSH添加到GitHub回到你的git bash中， 12git config --global user.name &quot;yourname&quot;git config --global user.email &quot;youremail&quot; 这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。 可以用以下两条，检查一下你有没有输对 12git config user.namegit config user.email 然后创建SSH,一路回车 1ssh-keygen -t rsa -C &quot;youremail&quot; 这个时候它会告诉你已经生成了.ssh的文件夹。在你的电脑中找到这个文件夹。 ssh，简单来讲，就是一个秘钥，其中，id_rsa是你这台电脑的私人秘钥，不能给别人看的，id_rsa.pub是公共秘钥，可以随便给别人看。把这个公钥放在GitHub上，这样当你链接GitHub自己的账户时，它就会根据公钥匹配你的私钥，当能够相互匹配时，才能够顺利的通过git上传你的文件到GitHub上。 而后在GitHub的setting中，找到SSH keys的设置选项，点击New SSH key把你的id_rsa.pub里面的信息复制进去。 在gitbash中，查看是否成功 1ssh -T git@github.com 6. 将hexo部署到GitHub这一步，我们就可以将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件_config.yml，翻到最后，修改为YourgithubName就是你的GitHub账户 1234deploy:type: gitrepo: https://github.com/YourgithubName/YourgithubName.github.io.gitbranch: master 这个时候需要先安装deploy-git ，也就是部署的命令,这样你才能用命令部署到GitHub。 1npm install hexo-deployer-git --save 然后 123hexo cleanhexo generatehexo deploy 其中 hexo clean清除了你之前生成的东西，也可以不加。hexo generate 顾名思义，生成静态文章，可以用 hexo g缩写hexo deploy 部署文章，可以用hexo d缩写 注意deploy时可能要你输入username和password。 部署成功后过一会儿就可以在http://yourname.github.io 这个网站看到你的博客了！！ 7. 设置个人域名现在你的个人网站的地址是 yourname.github.io，如果觉得这个网址逼格不太够，这就需要你设置个人域名了。但是需要花钱。 注册一个阿里云账户,在阿里云上买一个域名，我买的是fangzh.top，各个后缀的价格不太一样，比如最广泛的.com就比较贵，看个人喜好咯。 你需要先去进行实名认证,然后在域名控制台中，看到你购买的域名。 点 解析 进去，添加解析。 其中，192.30.252.153 和 192.30.252.154 是GitHub的服务器地址。注意，解析线路选择默认 ，不要像我一样选境外。这个境外是后面来做国内外分流用的,在后面的博客中会讲到。记得现在选择 默认 ！！ 登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名fangzh.top 然后在你的博客文件source中创建一个名为CNAME文件，不要后缀。写上你的域名。 最后，在gitbash中，输入 123hexo cleanhexo ghexo d 过不了多久，再打开你的浏览器，输入你自己的域名，就可以看到搭建的网站啦！ 接下来你就可以正式开始写文章了。 1hexo new newpapername 然后在source/_post中打开markdown文件，就可以开始编辑了。当你写完的时候，再 123hexo cleanhexo ghexo d 就可以看到更新了。 第二部分hexo的基本配置，更换主题，实现多终端工作，以及在coding page部署实现国内外分流。 1. hexo基本配置在文件根目录下的_config.yml，就是整个hexo框架的配置文件了。可以在里面修改大部分的配置。详细可参考官方的配置描述。 网站 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述 author 您的名字 language 网站使用的语言 timezone 网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York,Japan, 和 UTC 。 其中，description主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，通常建议在其中包含您网站的关键词。author参数用于主题显示文章的作者。 网址 参数 描述 url 网址 root 网站根目录 permalink 文章的 永久链接 格式 permalink_defaults 永久链接中各部分的默认值 在这里，你需要把url改成你的网站域名。 permalink，也就是你生成某个文章时的那个链接格式。 比如我新建一个文章叫temp.md，那么这个时候他自动生成的地址就是http://yoursite.com/2018/09/05/temp。 以下是官方给出的示例，关于链接的变量还有很多，需要的可以去官网上查找 [永久链接](https://hexo.io/zh-cn/docs/permalinks) 。 参数 结果 :year/:month/:day/:title/ 2013/07/14/hello-world :year-:month-:day-:title.html 2013-07-14-hello-world.html :category/:title foo/bar/hello-world 再往下翻，中间这些都默认就好了。 12345678theme: landscape # Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:type: gitrepo: branch: [branch] theme就是选择什么主题，也就是在theme这个文件夹下，在官网上有很多个主题，默认给你安装的是lanscape这个主题。当你需要更换主题时，在官网上下载，把主题的文件放在theme文件夹下，再修改这个参数就可以了。 接下来这个deploy就是网站的部署的，repo就是仓库(Repository)的简写。branch选择仓库的哪个分支。这个在之前进行githubpage部署的时候已经修改过了，不再赘述。而这个在后面进行双平台部署的时候会再次用到。 Front-matterFront-matter 是文件最上方以 --- 分隔的区域，用于指定个别文件的变量，举例来说： 123title: Hello Worlddate: 2013/7/13 20:46:25--- 下是预先定义的参数，您可在模板中使用这些参数值并加以利用。 参数 描述 layout 布局 title 标题 date 建立日期 updated 更新日期 comments 开启文章的评论功能 tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址 其中，分类和标签需要区别一下，分类具有顺序性和层次性，也就是说 Foo, Bar 不等于 Bar, Foo；而标签没有顺序和层次。 12345categories:- Diarytags:- PS3- Games layout（布局）当你每一次使用代码 1hexo new paper 它其实默认使用的是post这个布局，也就是在source文件夹下的_post里面。 Hexo 有三种默认布局：post、page 和 draft，它们分别对应不同的路径，而您自定义的其他布局和 post 相同，都将储存到source/_posts 文件夹。 布局 路径 post source/_posts page source draft source/_drafts 而new这个命令其实是： 1hexo new [layout] 只不过这个layout默认是post罢了。 page如果你想另起一页，那么可以使用 1hexo new page board 系统会自动给你在source文件夹下创建一个board文件夹，以及board文件夹中的index.md，这样你访问的board对应的链接就是http://xxx.xxx/board draftdraft是草稿的意思，也就是你如果想写文章，又不希望被看到，那么可以 1hexo new draft newpage 这样会在source/_draft中新建一个newpage.md文件，如果你的草稿文件写的过程中，想要预览一下，那么可以使用 1hexo server --draft 在本地端口中开启服务预览。 如果你的草稿文件写完了，想要发表到post中， 1hexo publish draft newpage 就会自动把newpage.md发送到post中。 2. 更换主题到这一步，如果你觉得默认的landscape主题不好看，那么可以在官网的主题中，选择你喜欢的一个主题进行修改就可以啦。点这里 这里有200多个主题可以选。不过最受欢迎的就是那么几个，比如[NexT主题](https://github.com/theme-next/hexo-theme-next)，非常的简洁好看，大多数人都选择这个，关于这个的教程也比较多。不过我选择的是icarus这个主题 直接在github链接上下载下来，然后放到theme文件夹下就行了，然后再在刚才说的配置文件中把theme换成那个主题文件夹的名字，它就会自动在theme文件夹中搜索你配置的主题。 而后进入icarus这个文件夹，可以看到里面也有一个配置文件_config.xml，貌似它默认是_config.xml.example，把它复制一份，重命名为_config.xml就可以了。这个配置文件是修改你整个主题的配置文件。 menu（菜单栏）也就是上面菜单栏上的这些东西。 其中，About这个你是找不到网页的，因为你的文章中没有about这个东西。如果你想要的话，可以执行命令 1hexo new page about 它就会在根目录下source文件夹中新建了一个about文件夹，以及index.md，在index.md中写上你想要写的东西，就可以在网站上展示出来了。 如果你想要自己再自定义一个菜单栏的选项，那么就 1hexo new page yourdiy 然后在主题配置文件的menu菜单栏添加一个 Yourdiy : /yourdiy，注意冒号后面要有空格，以及前面的空格要和menu中默认的保持整齐。然后在languages文件夹中，找到zh-CN.yml，在index中添加yourdiy: '中文意思'就可以显示中文了。 customize(定制)在这里可以修改你的个人logo，默认是那个hueman，在source/css/images文件夹中放入自己要的logo，再改一下url的链接名字就可以了。 favicon是网站中出现的那个小图标的icon，找一张你喜欢的logo，然后转换成ico格式，放在images文件夹下，配置一下路径就行。 social_links ，可以显示你的社交链接，而且是有logo的。 widgets(侧边栏)侧边栏的小标签，如果你想自己增加一个，比如我增加了一个联系方式，那么我把communication写在上面，在zh- CN.yml中的sidebar，添加communication: '中文'。 然后在hueman/layout/widget中添加一个communicaiton.ejs，填入模板： 12345678&lt;% if (site.posts.length) { %&gt; &lt;div class=&quot;widget-wrap widget-list&quot;&gt; &lt;h3 class=&quot;widget-title&quot;&gt;&lt;%= __('sidebar.communiation') %&gt;&lt;/h3&gt; &lt;div class=&quot;widget&quot;&gt; &lt;!--这里添加你要写的内容--&gt; &lt;/div&gt; &lt;/div&gt;&lt;% } %&gt; search(搜索框)默认搜索框是不能够用的， you need to install hexo-generator-json-content before using Insight Search 它已经告诉你了，如果想要使用，就安装这个插件。 comment(评论系统)这里的多数都是国外的，基本用不了。这个valine好像不错，还能统计文章阅读量，可以自己试一试，链接。 miscellaneous(其他)这里我就改了一个links，可以添加友链。注意空格要对！不然会报错！ 总结：整个主题看起来好像很复杂的样子，但是仔细捋一捋其实也比较流畅， languages: 顾名思义 layout：布局文件，其实后期想要修改自定义网站上的东西，添加各种各样的信息，主要是在这里修改，其中comment是评论系统，common是常规的布局，最常修改的在这里面，比如修改页面head和footer的内容。 scripts：js脚本，暂时没什么用 source：里面放了一些css的样式，以及图片 3. git分支进行多终端工作问题来了，如果你现在在自己的笔记本上写的博客，部署在了网站上，那么你在家里用台式机，或者实验室的台式机，发现你电脑里面没有博客的文件，或者要换电脑了，最后不知道怎么移动文件，怎么办？ 在这里我们就可以利用git的分支系统进行多终端工作了，这样每次打开不一样的电脑，只需要进行简单的配置和在github上把文件同步下来，就可以无缝操作了。 机制机制是这样的，由于hexo d上传部署到github的其实是hexo编译后的文件，是用来生成网页的，不包含源文件。 也就是上传的是在本地目录里自动生成的.deploy_git里面。 其他文件 ，包括我们写在source 里面的，和配置文件，主题文件，都没有上传到github 所以可以利用git的分支管理，将源文件上传到github的另一个分支即可。 上传分支首先，先在github上新建一个hexo分支，如图： 然后在这个仓库的settings中，选择默认分支为hexo分支（这样每次同步的时候就不用指定分支，比较方便）。 然后在本地的任意目录下，打开git bash， 1git clone git@github.com:Nameless1732/Nameless1732.github.io.git 将其克隆到本地，因为默认分支已经设成了hexo，所以clone时只clone了hexo。 接下来在克隆到本地的Nameless1732.github.io中，把除了.git 文件夹外的所有文件都删掉 把之前我们写的博客源文件全部复制过来，除了.deploy_git。这里应该说一句，复制过来的源文件应该有一个.gitignore，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下，表示这些类型文件不需要git： 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 注意，如果你之前克隆过theme中的主题文件，那么应该把主题文件中的.git文件夹删掉，因为git不能嵌套上传，最好是显示隐藏文件，检查一下有没有，否则上传的时候会出错，导致你的主题文件无法上传，这样你的配置在别的电脑上就用不了了。 而后 123git add .git commit –m &quot;add branch&quot;git push 这样就上传完了，可以去你的github上看一看hexo分支有没有上传上去，其中node_modules、public、db.json已经被忽略掉了，没有关系，不需要上传的，因为在别的电脑上需要重新输入命令安装。 这样就上传完了。 更换电脑操作一样的，跟之前的环境搭建一样， 安装git 1sudo apt-get install git 设置git全局邮箱和用户名 12git config --global user.name &quot;yourgithubname&quot;git config --global user.email &quot;yourgithubemail&quot; 设置ssh key 12345ssh-keygen -t rsa -C &quot;youremail&quot;#生成后填到github和coding上（有coding平台的话）#验证是否成功ssh -T git@github.comssh -T git@git.coding.net #(有coding平台的话) 安装nodejs 12sudo apt-get install nodejssudo apt-get install npm 安装hexo 1sudo npm install hexo-cli -g 但是已经不需要初始化了， 直接在任意文件夹下， 1git clone 然后进入克隆到的文件夹： 123cd xxx.github.ionpm installnpm install hexo-deployer-git --save 生成，部署： 12hexo ghexo d 然后就可以开始写你的新博客了 1hexo new newpage Tips: 不要忘了，每次写完最好都把源文件上传一下 123git add .git commit –m &quot;xxxx&quot;git push 如果是在已经编辑过的电脑上，已经有clone文件夹了，那么，每次只要和远端同步一下就行了 1git pull 第三部分hexo添加各种功能，包括搜索的SEO，阅读量统计，访问量统计和评论系统等。 本文参考了:visugar.com这里面说的很详细了。 1. SEO优化推广是很麻烦的事情，怎么样别人才能知道我们呢，首先需要让搜索引擎收录你的这个网站，别人才能搜索的到。那么这就需要SEO优化了。 SEO是由英文Search Engine Optimization缩写而来，中文意译为“搜索引擎优化”。SEO是指通过站内优化比如网站结构调整、网站内容建设、网站代码优化等以及站外优化。 百度seo刚建站的时候是没有搜索引擎收录我们的网站的。可以在搜索引擎中输入site:&lt;域名&gt; 来查看一下。 1. 登录百度站长平台添加网站 登录百度站长平台，在站点管理中添加你自己的网站。 验证网站有三种方式：文件验证、HTML标签验证、CNAME验证。 第三种方式最简单，只要将它提供给你的那个xxxxx使用CNAME解析到xxx.baidu.com就可以了。也就是登录你的阿里云，把这个解析填进去就OK了。 2. 提交链接 我们需要使用npm自动生成网站的sitemap，然后将生成的sitemap提交到百度和其他搜索引擎 12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save 这时候你需要在你的根目录下_config.xml中看看url有没有改成你自己的： 重新部署后，就可以在public文件夹下看到生成的sitemap.xml和baidusitemap.xml了。 然后就可以向百度提交你的站点地图了。 这里建议使用自动提交。 自动提交又分为三种：主动推送、自动推送、sitemap。 可以三个一起提交不要紧，我选择的是后两种。 自动推送：把百度生成的自动推送代码，放在主题文件/layout/common/head.ejs的适当位置，然后验证一下就可以了。 sitemap：把两个sitemap地址，提交上去，看到状态正常就OK了。 ps: 百度收录比较慢，慢慢等个十天半个月再去site:&lt;域名&gt;看看有没有被收录。 google的SEO流程一样，google更简单，而且收录更快，进入[google站点地图](https://search.google.com/search-console/sitemaps?resource_id=http://fangzh.top/&amp;hl=zh-CN)，提交网站和sitemap.xml，就可以了。 如果你这个域名在google这里出了问题，那你就提交 yourname.github.io，这个链接，效果是一样的。 不出意外的话一天内google就能收录你的网站了。 其他的搜索，如搜狗搜索，360搜索，流程是一样的，这里就不再赘述。 2. 评论系统评论系统有很多，但是很多都是墙外的用不了，之前说过这个valine好像集成在hueman和next主题里面了，但是我还没有研究过，我看的是visugar这个博主用的来比力评论系统，感觉也还不错。 来比力官网，注册好后，点击管理页面，在代码管理中找到安装代码： 获取安装代码后，在主题的comment下新建一个文件放入刚刚那段代码，再找到article文件，找到如下代码，若没有则直接在footer后面添加即可。livebe即为刚刚所创文件名称。 1&lt;%- partial('comment/livebe') %&gt; 然后可以自己设置一些东西： 还可以设置评论提醒，这样别人评论你的时候就可以及时知道了。 3. 添加百度统计百度统计可以在后台上看到你网站的访问数，浏览量，浏览链接分布等很重要的信息。所以添加百度统计能更有效的让你掌握你的网站情况。 百度统计，注册一下，这里的账号好像和百度账号不是一起的。 照样把代码复制到head.ejs文件中，然后再进行一下安装检查，半小时左右就可以在百度统计里面看到自己的网站信息了。 4. 文章阅读量统计leanCloudleanCloud，进去后注册一下，进入后创建一个应用： 在存储中创建Class，命名为Counter, 然后在设置页面看到你的应用Key，在主题的配置文件中： 1234leancloud_visitors:enable: trueapp_id: 你的idapp_key: 你的key 在article.ejs中适当的位置添加如下，这要看你让文章的阅读量统计显示在哪个地方了， 1阅读数量:&lt;span id=&quot;&lt;%= url_for(post.path) %&gt;&quot; class=&quot;leancloud_visitors&quot; data-flag-title=&quot;&lt;%- post.title %&gt;&quot;&gt;&lt;/span&gt;次 然后在footer.ejs的最后，添加： 12345678910111213141516171819202122232425262728293031323334353637383940&lt;script src=&quot;//cdn1.lncld.net/static/js/2.5.0/av-min.js&quot;&gt;&lt;/script&gt;&lt;script&gt; var APP_ID = '你的app id'; var APP_KEY = '你的app key'; AV.init({ appId: APP_ID, appKey: APP_KEY }); // 显示次数 function showTime(Counter) { var query = new AV.Query(&quot;Counter&quot;); if($(&quot;.leancloud_visitors&quot;).length &gt; 0){ var url = $(&quot;.leancloud_visitors&quot;).attr('id').trim(); // where field query.equalTo(&quot;words&quot;, url); // count query.count().then(function (number) { // There are number instances of MyClass where words equals url. $(document.getElementById(url)).text(number? number : '--'); }, function (error) { // error is an instance of AVError. }); } } // 追加pv function addCount(Counter) { var url = $(&quot;.leancloud_visitors&quot;).length &gt; 0 ? $(&quot;.leancloud_visitors&quot;).attr('id').trim() : 'icafebolger.com'; var Counter = AV.Object.extend(&quot;Counter&quot;); var query = new Counter; query.save({ words: url }).then(function (object) { }) } $(function () { var Counter = AV.Object.extend(&quot;Counter&quot;); addCount(Counter); showTime(Counter); });&lt;/script&gt; 重新部署后就可以了。 5. 引入不蒜子访问量和访问人次统计不蒜子的添加非常非常方便，不蒜子 在footer.ejs中的合适位置，看你要显示在哪个地方，添加： 1234&lt;!--这一段是不蒜子的访问量统计代码--&gt;&lt;script async src=&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;&lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;本站总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次 &amp;nbsp; &lt;/span&gt;&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt;访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次&lt;/span&gt; 就可以了。 总结到这里就基本做完了。其实都是参考别的博主的设置的，不一定仅限于hueman主题，其他主题的设置也是大体相同的，所以如果你希望设置别的主题，那么仔细看一下这个主题的代码结构，也能够把上边的功能添加进去。 多看看别的博主的那些功能，如果有你能找到自己喜欢的功能，那么好好发动搜索技能，很快就能找到怎么做了。加油吧！","link":"/Article/hexo/"},{"title":"计算机网络复习","text":"注：这是一份临近考试自己复习、所总结的东西，方面自己查阅。 以下内容有王道考研课件截图、本人学校课堂习题、以及同学们分享的笔记。 物理层OSI与TCP/IP协议栈协议​ 网络中对等实体数据交换而建立的规则、约定 接口​ 上层使用下层服务的入口 服务下层为相邻上层提供的功能调用 分层概念为什么要分层？ ​ 发起通信的计算机 必须将数据链路激活​ 告诉网络如何识别目的主机​ 能够检测网络连接是否正常​ 确保差错与流量控制 OSI与TCP/IP OSI参考模型 TCP/IP参考模型 TCP/IP协议 ​应用层 ​表示层 应用层 HTTP/FTP/DNS ​会话层 ​传输层 传输层 TCP UDP ​网络层 网络层 IP ​数据链路层 网络接口层 Ethernet STM Frame Relay 物理层 TCP/IP 参考模型各层的主要功能 应用层 使用网络服务应用进程、与用户交互 传输层 TCP可靠传输 UDP不可靠传输 与 拥塞控制等 网络层 IP数据报的传输 路由规划等 网络接口层 数据成帧 透明传输 差错控制 编码技术单极性不归零高1 低0 曼彻斯特编码高-&gt;低 1 低-&gt;高 0 差分曼彻斯特编码同 1 异 0 香农与奈氏奈氏准则适用条件：理想情况下无噪声干扰、避免码间串扰、码元传输速率限制 V 为 一个码元携带的比特位数 香农定理适用条件：理想情况下有噪声干扰、避免码间串扰、码元传输速率限制 奈式准则与香农定理 例题 数据链路层装配成帧(透明传输)字符计数法 字节填充法在数据中遇到帧头帧尾 在前面夹ESC````遇到ESC在前面加ESCesc键的ascii码十进制表示为27 比特填充法帧头帧尾 01111110数据遇到5个1就在后面加一个0 违规编码法利用曼切斯特、差分曼切斯特编码特性、表示非法编码作为帧头 帧尾 差错控制检错编码 奇偶校验 举例: 010101奇校验 0010101 (加0为奇)偶校验 1010101 (补1为偶) CRC校验 例如 : 传输数据为 1101 0110 11 生成多项式为 10011解: 生成多项式最高位 2^4 则阶r=4 在传输数据后面追加[r]个零计算：同为0 异为1 异或运算 123456789101112131415 1100 00 1010 /-------------------10011 / 1101 0110 11 0000 1001 1 -------------------- 0100 11 100 11 -------------------- 10 11 0 10 01 1 -------------------- 10 100 10 011 -------------------- 1110 (FCS 帧检验序列) 校验方法接收方在 传输的数据后追加FCS,1101 0110 1111 10,使用生成多项式进行同样的操作如果余数为 0 则帧检验无误 ​ 纠错编码 海明码校验 海明不等式 2^r &gt;= k+r+1距离: 传输数据 D=101101则海明校验码长度为 2^r &gt;= 6+r+1 min(r)=4 二进制位 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010数据位 1 2 3 4 5 6 7 8 9 10代码 P1 P2 D1 P3 D2 D3 D4 P4 D5 D6实际值 1 0 1 1 0 1 如何求 P1 P2 P3 P4 呢?看P1 即2^0 找0位与其相同的数据位 同为 0 | 异为 1即 P1⊕D1⊕D2⊕D4⊕D5=0 0 1 0 1 0 =0依次求出 P1 P2 P3 P4 接收方怎样纠错?例如P1⊕D1⊕D2⊕D4⊕D5=1P2⊕D1⊕D3⊕D4⊕D6=0P3⊕D2⊕D3⊕D4=1P4⊕D5⊕D6=0 二进制: 1010 B = 5 D则 说明 D5 有错 流量控制 (滑动窗口) 目的 解决的问题 接受速度与发送速度匹配问题减少传输出错与资源浪费 简单停止等待协议 发送方每发送一帧就等待接收方的确认、即发送就等待确认具有超时重传特性 ： 即超过RTT 没有接收到确认 则再次重传缺点 ： 发送方浪费太多时间、大部分时间都花在了RTT时间上了，信道利用率太低 计算信道利用率与信道吞吐率信道利用率 = (L/C)/TL: T时间内发送比特数量C: 发送方数据传输率T: 发送周期 注意事项： 关于T的计算与(L/C)的计算 三种协议有所不同关于 L/C的计算 通常 (L/C) = 帧长/发送速率退后N与选择重传协议需要与其发送窗口大小相乘 需要会计算算窗口大小 T的计算 (L/C)+2t t为传播延时如果使用了捎带确认则T =(L/C)*2+2t 信道吞吐率= 信道利用率*发送方的发送速率 退后N帧协议窗口大小 &lt;=2^n -1 n为序号个数 选择重传协议窗口大小 &lt;=2^(n-1) n为序号个数 例题 计算退回N 需要多少个序号 介质访问控制 多路复用 静态划分1.1 频分多路复用多个节点之间使用不同的频率 互不干扰1.2 时分多用每个节点只能在自己能发的时间片内发送1.3 波分多路复用每个信道采用不同的波长1.4 码分多路复用(重点) ​​ 每个站点有自己的芯片序列​ 例如​ 站点一 -1 1 -1 -1 1 -1 （1为1 -1为0）​ 站点二 1 1 1 1 1 -1​ 站点发送1 则发送芯片序列 发送0 则发送芯片序列的反码​ 两向量正交​​ 站点一 发送 0​ 站点二 发送 1​ 则 1 -1 1 1 -1 1​ 1 1 1 1 1 -1​ 相加​ 2 0 2 2 0 0​​ 解码 -1：发送了0 1：发送了1 0：没有发送​ 站点一 [2 0 2 2 0 0]*[-1 1 -1 -1 1 -1] =(-2-2-2)/6=-6/6=-1 =&gt; 0(即发送了0)​ 站点二 同理 动态划分ALOHA协议想发就发、出错超时重传 CSMA协议 &gt;载波监听先听再发、出错超时重传 CSMA/CD协议 &gt;载波监听 冲突检测先听再发、边传边听即进行碰撞检测、出错后根据情况重传（最小帧长 / 数据传输速率） &gt;= 2T最小帧长 = 2T * 数据传输率 发生碰撞则 什么时机再进行发送 r=[0,1,3,…,2^k - 1] max( k )=10 回避时间 r*2T max( r )=16 注：以太网最小帧长为64B例题：计算最小帧长 ​​ 例1、在1km长电缆、建立1GB/s的网络、电缆信号速度为200000km/s​ 求最小帧长？​ 答：​ 最小帧长 &gt;= 数据传输率 * 2T​ (1GB/s)2(1km/200000km/s)​​ 例2、帧传播速度 10Mb/s 距离2.5km 信号传播速率 210^8 m/s​ 求最短帧长​ 答：​ 最短帧长 &gt;= 数据传输率 * 2T​ (10Mb/s)2(2.5km/(210^8 m/s)) CSMA/CA协议 &gt;载波监听 冲突避免使用于无线局域网络先检测就没有人在发 没有则发送 自己要发的信号 得到接受方确认 然后就可以发了为什么要有CSMA/CA? ​​ 对于无线局域网而言​ 无法做到360度 全面检测​ 隐蔽站：当A C 同时检测不到信号、都认为信道空闲、同时向B发送数据就会造成冲突​ CSMA/CA工作原理​ 1、发送方检测信道是否空闲​ 2、如果空闲则发送RTS​ 3、接收方收到RTS、发送CTS​ 4、发送方接收到CTS后、开始发送数据帧 轮询介质访问控制 轮询协议主节点依次询问 其他节点要不要发 当被询问才能发 令牌协议通过特定硬件 传递令牌 拥有令牌才能发送 MAC帧协议 （直接去看协议分析题目） 通常指以太网MAC帧 采用CSMA/CD介质访问控制 曼彻斯特编码最小帧长：64BMAC帧格式 目的地址 源地址 类型 数据（IP数据报） FCS 6 6 2 46~1500 4 无线局域网MAC帧格式 802.11MAC帧头 帧控制 生存周期ID 地址1 地址2 地址3 序列控制 地址4 2 2 6 6 6 2 6 地址1：AP1 地址2：AP2 地址3：目标mac地址 地址4：源mac地址 PPP协议协议 (Point-to-Point Protocol)拨号基本都是PPP协议 仅支持全双工链路 协议不需要满足的需求纠错 流量控制 序号 不支持多点线路 协议三个组成部分 将IP数据报封装到串行链路 链路控制协议LCP:建立并维护数据链路连接 网络控制协议NCP：PPP支持多种网络层协议、每个网络层都要一个NCP配置。 HDLC协议目标：简答 网桥网桥、交换机、路由器 可以隔离 冲突域 网桥帧转发 算法 Hash表映射 网络层报文交换与分组交换​​ 分组交换效率更高​ 例​ 源报文10000bit 链路传输速率 10000bps​ 分组 每组10bit​​ 经过两个交换设备​​ 报文交换​ 10000bit/1000bps=10s​ 10s3=30s​​ 分组交换​ 10bit/1000bps=0.01s​ 10+0.01s2=10.02s ​ 数据报方式与虚电路方式​​ 数据报方式 为 网络层 提供无连接服务​ 不同的分组、可能经过的传输路径不同、不事先分配固定路径​​ 虚电路方式 为 网络层 提供连接服务​ 为分组的传输分配路径、然后使用确定好的路径、系列分组传输路径相同、传输结束后拆除连接。 ​ 路由算法与路由协议​​ 静态路由与动态路由​ 动态路由​ OSPF (链路状态路由算法)​ RIP (距离向量路由算法) RIP 仅和相邻路由器交换信息 路由器交换的信息是自己的路由表 固定时间交换一次、如果长时间没有接收到则认为相邻节点挂了、更新自己的路由表 距离向量题要会算目标 距离 下一跳 距离向量路由算法 例题 OSPF协议 (必须掌握 迪彻斯卡拉 单源最短路径算法)广度优先进行扩散重点掌握最短路径算法算法：每一步都选距离起点最近的节点、并检查此节点到没有走过的与其相邻的节点是否可以利用、优化相邻节点的距离。 例题 BGP协议 （期末大概率不考）每个网络有自己BGP发言人、有发言人向其他路由进行传递消息 IP数据报 格式 （直接看协议分析题）​​ 0 4 8 16 19 24 31​ | 版本 | 首部长度 | 分区服务 | 总长度 |​ | 标识 |标志 | 片偏移 |​ | 生存时间 | 协议 | 首部检验和 |​ | 源地址 |​ | 目标地址 |​ | 可选字段| 填充 |​ 数据部分​​ 标志位 X DF MF DF 0/1 禁止分片/允许分片​ MF 0/1 后面没有分片了（最后一片）/ 后面还有​​ 偏移量：此片的第一位相对于整体数据开始的差 即偏移量 IP划分重点记住 A B C类网络号的长度 以及开头的0 、10、110 重点记住在子网划分时 子网络号不能全为0 或者 全为1 期末考试考试几率不大 如果有选择题可能容易考察 NAT网络地址转换因为IP公网IP地址有限 则 连接内网和外网、私有IP发送数据要找个代理（共有IP）用其IP地址将数据与外面公网进行收发通过子网划分的配和 同一个公网IP地址可以 供多个子网进行使用 子网划分​​ 网络号 | 主机号 |​ 网络号 |子网号 | 主机号 |​ 子网号是否可以全零全一看情况​ 主机号不能全零或者全一 子网掩码子网掩码就是配和IP地址计算出 子网地址的子网最大数量与什么有关系 IP地址的从右向左数 到非零为止的位数 可以进行子网的划分 ​​ 以知IP地址为 144.14.72.24 子网掩码为 255.255.192.0​ 求 网络地址​ 128 64 32 16 8 4 2 1​ ——————–|—————-​ 1 1 1 1 1 1 1 1​​ IP地址 144.14.72.24/18​ 1 0 0 1 0 0 0 0 . 0 0 0 0 1 1 1 0 . 0 1 0 0 1 0 0 0 . 0 0 0 1 1 0 0 0​ 子网掩码 255.255.192.0​ 1 1 1 1 1 1 1 1 . 1 1 1 1 1 1 1 1 . 1 1 0 0 0 0 0 0 . 0 0 0 0 0 0 0 0​ 与运算————————————————————————-​ 1 0 0 1 0 0 0 0 . 0 0 0 0 1 1 1 0 . 0 1 0 0 0 0 0 0 . 0 0 0 0 0 0 0 0​ 结果 144 . 14 . 64 . 0​​ 子网号为 : 144.14.64.0 例题 (IP/子网掩码 求下一跳问题) 例题 (设计题、子网划分)重点： A B C类IP地址的网络号位数 8 16 24 子网号不能全为0 或者 全为 1 子网主机号 不能全为0 或者 全为 1 ARP协议（可能靠简答题）简略回答：ARP是一种高速缓存技术、将MAC地址与网络IP地址形成映射、 保存到自己本机、解决更好的解决下一跳走哪里的问题 传输层知识点端口号​​ 端口号为16bit 共有65536个端口号​ 01023 熟知端口号​ 102449151 没有熟知进程使用​ 49152~65535 动态选择的端口号 TCP UDP​​ TCP:可靠传输 可以分段​ UDP：不可靠传输 不可以分段 TCP 报文 （主要用于报文分析题目 不要盲目死记） TCP 三次握手与四次挥手三次握手​​ 第一步: 客户端发送连接请求报文段，无应用层数据​ SYN=1 seq=x(随机) 并​ 第二步: 服务端为TCP连接分配缓存和变量、并向客户端响应 确认报文段、允许连接，无应用层数据​ ACK=1 SYN=1 seq=y(随机) ack=x+1​ 第三步: 客户端为TCP连接分配缓存和变良,向服务端响应确认、可以携带应用层数据​ ACK=1 seq=x+1 ack=y+1 四次挥手 连接释放​​ 第一步：客户端发送连接释放请求​ FIN=1 seq=u​ 第二步： 服务器响应确认报文​ ACK=1 seq=v ack=u+1​ 第三步： 服务端发完数据，就发出连接释放报文、主动关闭TCP连接​ FIN=1 ACK=1 seq=w ack=u+1​ 第四步： 客户端回送一个确认报文段，再等到时间等待计时器设置的2MSL（最长报文段寿命）后，​ 连接彻底关闭​ ACK=1 seq=u+1 ack=w+1 TCP流量控制 拥塞控制算法作用 ： 试探网络传输能力 慢开始 拥塞避免 快重传 快恢复 例题 （拥塞算法） 报文分析 （记住就完了） 来自己练一下下面这个 应用层知识点 域名与DNS解析 邮件STMP 与 POP3STMP发邮件 25端口POP3协议收邮件 110端口 FTPTCP控制连接请求 21端口文件传输 20端口 HTTP协议 （请求报文与响应报文） 应用层综合题目","link":"/Article/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%A4%8D%E4%B9%A0/"},{"title":"操作系统复习","text":"前言 本篇文章的内容结合了哈工大李治军老师操作系统课程，王道考研操作系统的资料以及学习了B站CodeSheep的一次知识梳理，并且为了便于理解学习，增加了个人的一些解释。总之，概括而言对于开发人员来说，操作系统需要下四个方面进行学习：进程/线程；并发/锁；内存管理与调度；I/O原理，本文也将围绕这几点逐渐深入。 一、常用术语总结 名词 概念 PCB 进程控制块（PCB Process Control Block）,系统中存放、管理和控制进程信息的数据结构称为 TCB 线程控制块 FCB 文件控制块 PID 进程ID(Process ID) PSW 程序状态字寄存器，用于存放PC、IR等的信息 PC 程序计数器，存放下一条指令地址 IR 指令寄存器，存放到当前进行的指令 半双工 半双工和全双工是计算机网络中的概念，意思是通讯同一时间只允许一方发送数据(对讲机) 全双工 通信允许两方向上同时传输数据(电话) P操作 来自荷兰语proveren，代表wait原语，通常使用P(S)代替wait(S) V操作 来自荷兰语verhogen，代表原语signal,通常使用V(S)代替signal(S) 用户态 一般的操作系统对执行权限进行分级，分别为用保护态和内核态。用户态相较于内核态有较低的执行权限，很多操作是不被操作系统允许的，从而保证操作系统和计算机的安全。 内核态 内核态相当于一个介于硬件与应用之间的层，可以进行硬件的调度、使用，可以执行任何cpu指令，也可以引用任何内存地址，包括外围设备, 例如硬盘,网卡，权限等级最高。 用户态内核态切换 三种情况下，用户态会转换到内核态，系统调用、程序异常(例如/0，内存资源耗尽等)、来自外围设备的中断 系统调用/程序接口 用户程序通过系统调用的方式才能对硬件进行使用，或者说操作系统将使用硬件的接口提供给用户程序 中断 中断是操作系统内核程序夺取cpu的唯一途径，或者说用户程序调用内核代码的唯一途径，因为在一般情况下，操作系统会将cpu使用权交给应用程序。 二、进程、线程在进程、线程这一章节除了会讲进程、线程的概念，也会穿插，并发，锁。 2.1 进程进程由PCB(进程控制块)组成,包含了PID、资源分配情况、进程运行情况。 对用户而言，我们能看到 一个个PID ，而对操作系统而言，底层需要处理的是 一个个PCB 。 下图是通过任务管理器的用户视角下的进程。 2.1.1 进程控制进程控制相关的原语：创建、终止、阻塞、唤醒、切换。也就是说我们通过原语进行进程控制，原语的执行具有原子性，不允许被中断，原语的实现可以通过“关中断指令”和“开中断指令”实现。 进程状态：运行态，就绪态，阻塞态 进程状态转换的条件： 运行 -&gt; 阻塞 等待I/O或事件完成 运行 -&gt; 就绪 进程的CPU时间片用完 就绪 -&gt; 运行 获得了CPU的时间片 阻塞 -&gt; 就绪 I/O或事件完成 下图为进程控制的流程图。 2.1.2 进程的组织形式 在一个系统中，通常有数十、数百乃至数千个PCB。为了能对他们加以有效的管理，应该用适当的方式把这些PCB组织起来。 进程的组织形式分为两种：链接式和索引式 根据进程状态的不同，创建不同的索引表，可以通过指针可以通过索引表指到个PCB。 2.1.3 程序的执行 程序的状态字寄存器PSW用来存放两类信息：一类是体现当前指令执行结果的各种状态信息，如有无进位（CY位），有无溢出（OV位）等；另一类是存放控制信息，如允许中断(IF位)，跟踪标志（TF位）等。 相较于程序，进程是动态的而程序是静态的 2.1.4 进程通信 进程通信是指进程之间的信息交换。进程是分配系统资源的单位，因此各个进程拥有的内存地址相互独立，为了保证安全，一个进程不能直接访问另一个进程的地址空间，为了实现进程通信，操作系统提供了以下方法 进程通信方法：共享存储、信号量、消息队列/信箱、管道通信、套接字(这个在计算机网络有涉及相关知识，可以把套接字理解为一个窗口) 共享存储 消息传递 通过原语控制，进程1发送消息到消息缓冲队列或者信箱中，进程2从消息队列或者信箱中接收消息。 管道通信 2.2 线程 进程是资源分配的基本单位，线程是调度的基本单位，往往一个进程包含多个线程。线程并发，系统开销小，不需要切换系统资源。 线程可以分为用户级线程和内核级线程，早期如Unix只支持进程，不支持线程，所以当时的线程是由”线程库”实现的，用户将进程分为多个线程，放入线程库，但操作系统仍然是按照进程进行处理的。 这种用户级线程是由程序负责管理的，包括进行切换。这种切换方式 开销小，效率高 ，但下图当一个用户级线程被阻塞后，整个 进程都将会被阻塞 。 多线程模型 内核级线程是由操作系统完成调度的。 将n个用户级线程映射到m个内核级线程上（ n &gt;= m），优点是克服了多对一模型并发度不高的缺点，又克服了一对一模型中一个用户进程占用太多内核级线程，开销太大的缺点。 2.3 进程调度2.3.1 三种调度方式 调度的产生是因为系统资源有限，没办法同时处理所有进程，需要特定的规则分配执行顺序，从而有了调度 操作系统调度层次分为三类：高级调度、中级调度、低级调度。 高级调度：从外存的后备作业中挑选一个(多个)，建立相应的PCB，获得竞争处理的权力。 后面会讲到的虚拟内存技术出现后，为了提高系统的利用率和吞吐量，会将暂时等待的进程挂起到外存。 中级调度：能够决定哪个被挂起的进程重新回到内存中。 低级调度:从就绪队列中选取一个进程，使其能够被CPU处理。 三种调度方式的频率从低到高。 进程调度时机：当前运行进程主动放弃(进程中止、异常、主动请求阻塞)，被动放弃(时间片用完、更高优先级的进程进入就绪队列等) 2.3.2 调度算法评价指标 作为开发人员的话，大致了解以下即可，毕竟工作中几乎不会涉及到具体指标计算 CPU利用率 = 忙碌的时间/总时间 系统吞吐量 = 总共完成了多少道作业/总共花了多少时间 (简单理解就是一个完成作业的速度指标) 周转时间= 作业完成时间– 作业提交时间 平均周转时间 = 各作业周转时间之和/作业数 带权周转时间 = (作业完成时间– 作业提交时间)/作业实际运行的时间,这项指标一定是大于等于1的，越接近一则越好 平均带权周转时间 = 各作业带权周转时间之和/作业数 等待时间，指进程/作业处于等待处理机状态时间之和 响应时间，指从用户提交请求到首次产生响应所用的时间。例如键盘事件响应时间，鼠标点击响应时间 2.3.3 【重点】调度算法先来先服务FCFS：顾名思义，先来的进程先服务，主要从“公平”的角度考虑（类似于我们生活中排队买东西的例子）,非抢占式算法。 短作业优先SJF：最短的作业/进程优先得到服务，追求最少的平均等待时间，非抢占式算法。 最短剩余时间优先算法SRTN：每当有进程加入就绪队列改变时就需要调度，如果新到达的进程剩余时间比当前运行的进程剩余时间更短，则由新进程抢占处理机，当前运行进程重新回到就绪队列。是一种抢占式算法。 根据图示要区分最短剩余时间优先算法和短作业优先算法。 高响应比优先HRRN，相应比 = （等待时间+要求服务时间）/要求服务时间 ，是一种非抢占式的调度算法，只有当前运行的进程主动放弃CPU时（正常/异常完成，或主动阻塞），才需要进行调度。 以上方法适用于早期的批到处理系统，适用于交互式系统的调度算法。 适用于目前交互式系统的调度算法，在交互式操作系统中，可以将任务划分为前台任务(鼠标、键盘等任务)和后台任务，前台任务更关心响应事件，后台任务 时间片轮转调度算法RR：周期性切换PCB，各个PCB轮流使用CPU 最高优先级调度算法即前台进程优先级高于后台进程，系统进程优先级会高于用户进程，特点就是不公平，容易产生饥饿。 1973年关闭的MIT的IBM 7094时，发现有一个进程在1967年提交但一直未运行 2.4 实现进程互斥、同步、前驱如果要清晰讲述需要了解代码逻辑，对于非专业人员来说，必要性不大，这里就不详细罗列代码了。 2.4.1 硬件实现进程互斥进程互斥：当一个进程进入临界区后，另一个进程必须等待。 硬件能够实现进程互斥，有三种方式： 利用“开/关中断指令”实现 、 TSL指令 、 SWAP指令 。 2.4.2 信号量机制实现互斥、同步、前驱信号量机制：用户进程可以通过使用操作系统提供的原语对信号量进行操作，从而很方便的实现进程的互斥、同步、前驱。 不要一头钻到代码里，要注意理解信号量背后的含义，一个信号量对应一种资源 信，在操作系统中，信号量在源码中其实就对应着一个变量，代表着某种资源的数量，通过信号量可以实现进程互斥、同步、前驱。 进程同步的理解：要让各并发进程按要求有序地推进。若PCB2 的“代码4”要基于PCB1的“代码1”和“代码2”的运行结果才能执行，那么我们就必须保证“代码4”一定是在“代码2”之后才会执行。 前驱的理解：其实每一对前驱关系都是一个进程同步问题（需要保证一前一后的操作） 信号量保护：共享数据在没有保护的情况下，会出现安全问题，所以需要锁来进行保护，锁本质也是一个变量，用来保护信号量安全，那锁本身的安全谁来保护？我们不能在这里套娃对不对，所以锁是一种硬件原子指令，当要进入临界区时，上锁，离开临界区时解锁。下面的知识有一些扩展了。 在cpu芯片上有一个HLOCK Pin，可以通过发送指令来操作，将#HLOCKPin电位拉低，并持续到这条指令执行完毕，从而将总线锁住，这样同一总线上的其他CPU就不能通过总线来访问内存了。最开始这些功能是用来测试cpu的，后来被操作系统实现而封装成各种功能：关键代码段，信号量等。 2.5 死锁的概念与处理死锁的4个条件，缺一不可 互斥条件：对必须互斥使用的资源的争抢才会导致死锁 不剥夺条件：进程所获得的资源未使用完之前，不能被其他进程强行夺走，只能主动释放。 请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源又被其他进程占有，此时请求进程被阻塞，但又对自己有的资源保持不放。就像很窄的桥，两个人都要去对面，但谁又都无法让出位置来 循环等待条件：存在一种进程资源的循环等待，链中的每一个进程已获得的资源同时被下一个进程所请求。想象有一个闭环，闭环上每个人都需要下一个人手上的某个资源，那么所有人都没办法满足 下图就像我们的一个死锁 死锁的处理方式： 死锁预防 破坏互斥条件(创建一个队列，所有的请求都会被快速响应，然后队列逐渐将请求发送到处理器进行整理) 破坏不剥夺条件(进程的某个资源得不到满足时，就必须立刻释放所持有的资源) 破坏请求和保持条件(静态分配，进程在运行前就一次性申请全部的资源，不满足就不让允许，就像过桥时保证桥上没人才让通行，否则禁止通行) 破坏循环等待条件(资源编号，进程必须按照编号递增的顺序请求资源,这样就不会出现持有大资源请求小资源的情况，也就不会有循环的等待) (方式较多，这里简单列举一些案例) 死锁避免 银行家算法：进程提出资源申请时，先判断这次分配会不会导致系统进入不安全状态，如果会则不答应请求，让该进程阻塞。简而言之，请求不能大于手中的资源。这种算法也叫银行家算法。 死锁检测与恢复 检测方法通过死锁检测算法，下面以图的方式说明 能够消除所有边，就不会发生死锁如下图 不能消除所有边就会发生死锁 死锁恢复：资源剥夺将死锁的进程挂起，释放资源；撤销进程，直接将部分或者全部死锁进程撤销；进程回退，让进程回退的足以避免死锁的地方。 死锁忽略 顾名思义，忽略这个死锁，死锁概率本不高，就算出现了也只是局部的死锁，直接不管反而能提高资源利用率与整体运行的速度，也有个好听的名字叫鸵鸟算法，这样性能会提升不少，大部分操作系统也采用死锁忽略的策略。 三、内存内存：程序执行前要先放到内存中才能被CPU处理。计算机小白可能以为内存就是存储空间，其实不然。 地址：4G的内存空间，有4*230个字节，也就是232个字节，要表示全部的字节就需要 32位的二进制地址，这也就是我们以前流行的32位操作系统的由来。但目前内存基本上都是8G，16G，所以32位已经不能满足需求，当前主流是 64位操作系统，能表示2^64个字节，最高4,294,967,296G的内存空间，远远满足当前的任何个人电脑地址表达需求。 3.1 程序装入内存 可执行文件需要放在内存中才可以运行，程序中的指令是逻辑地址，而内存中的地址是物理地址。如何将逻辑地址转为物理地址？ 绝对装入 下图为可执行文件指令。 假如0-100的地址已经被占用，系统知道装入模块要从地址为100 的地方开始存放，编译时就将指令的绝对地址载入了可执行文件，即 静态重定位 编译、链接后的装入模块的地址都是从0开始的，即可执行文件指令地址不变，在装入内存时，逻辑地址全部+100，装入内存中。所以作业一旦装入就没办法再更改、申请内存空间。 动态重定位 又称动态运行时装入。编译、链接后的装入模块的地址都是从0开始的。装入程序把装入模块装入内存后，并不会立即把逻辑地址转换为物理地址，而是把地址转换推迟到程序真正要执行时才进行 。 这种方式需要一个重定位寄存器的支持 ，寄存器记录了初始进入的的地址即100，在程序运行时，会动态的将指令中的逻辑地址增加上初始地址。 3.2 链接方式 静态链接 在程序运行之前，先将各目标模块及它们所需的库函数连接成一个完整的可执行文件（装入模块），之后不再拆开。 装入时动态链接 装入时动态链接：将各目标模块装入内存时，边装入边链接的链接方式。 运行时动态链接 在程序执行中需要该目标模块时，才对它进行链接。其优点是物理地址便于修改和更新，便于实现对目标模块的共享。 3.3 内存管理操作系统需要负责内存空间的分配与回收、内存空间扩充、地址转换、内存保护。 分配与回收 分配在程序链接方式已经讲明白了，回收即对内存中的进程进行撤销、挂起等操作。 扩充 这会涉及到后面的虚拟内存技术，这也十分常见，比方说我们5G的程序如何运行在2G的内存上，这就需要扩充。 地址转换 在装入内存已经讲过。 内存保护 简单来说就是操作系统保护已经在内存上的进程不被干扰。一种方式是设置上下限寄存器存放进程上下界，如100-179被占用，其他进程不可使用这里的内存。另外一种方式是重定位寄存器与界地址寄存器，如下图 通过逻辑判断内存是否能执行某操作。 3.4 覆盖技术与交换技术技术产生的原因：程序所需的运行空间大于实际内存大小，需要让内存发挥出更大的作用。 覆盖技术：按照逻辑，将不可能同时被访问的程序段共享一个覆盖区 交换技术:内存紧张时，将部分进程暂时挂起(放置到外村)，将外村已具备运行条件的进程换入内存。 3.5 连续分配管理方式单一连续分配：用于早期操作系统，内存被划分为系统区和用户区，内存只能有一道用户程序，局限性不言而喻。 固定分区分配：整个用户空间划分为若干个固定大小的分区，在每个分区中只装入一道作业。 动态分区分配:不会预先划分内存分区，而是在进程装入内存时，根据进程的大小动态地建立分区。 动态分区分配会产生 外部碎片 ，外部碎片，是指内存中的某些空闲分区由于太小而难以利用。 3.6 动态分区分配算法 首次适应算法 每次都从低地址开始查找，找到第一个能满足大小的空闲分区 最佳适应算法 因此为了保证当“大进程”到来时能有连续的大片空间，可以尽可能多地留下大片的空闲区，即，优先使用更小的空闲区。 最坏适应算法 为了解决最佳适应算法的问题——即留下太多难以利用的小碎片，可以在每次分配时优先使用最大的连续空闲区，这样分配后剩余的空闲区就不会太小，更方便使用。 邻近适应算法 首次适应算法每次都从链头开始查找的。这可能会导致低地址部分出现很多小的空闲分区，而每次分配查找时，都要经过这些分区，因此也增加了查找的开销。如果每次都从上次查找结束的位置开始检索，就能解决上述问题。 综合来看 3.7 基本分页存储管理在3.6中的算法总是存在种种问题，在实际操作系统中，分页可以解决内存分区导致的效率、碎片问题。 图示 页框: 每个分区就是一个“页框”（页框=页帧=内存块=物理块=物理页面） 页面:将进程的逻辑地址空间也分为与页框大小相等的一个个部分，每个部分称为一个“页”或“页面” 页框不能太大，否则可能产生过大的内部碎片导致浪费。 页框和页面总是记混，技巧其实很简单只要记住页面放在页框中，就再也不会记混了。 页表:操作系统要为每个进程建立一张页表。页表通常存在PCB,这里概念不清的话可能会有疑惑为什么PCB中会有页表，仔细阅读前面的部分，PCB中不光存放进程，还存放着进程信息、PID、IO情况等多个内容。 页表记录进程页面和实际存放的内存块之间的映射关系 页表项所占字节：假设某系统物理内存大小为4GB，页面大小为4KB，则有2^32 / 2^12 = 2^20个内存块，那么每一个块号就需要20bit,至少3字节来表示。由于页号是隐含的，因此每个页表项占3B。 同样的，有进程和内存，就有地址转换的问题进程的页号是逻辑地址，内存的页框号是物理地址，中间需要计算页内偏移量。 页号= 逻辑地址/ 页面长度（取除法的整数部分） 页内偏移量= 逻辑地址% 页面长度（取除法的余数部分） 逻辑地址A 对应的物理地址= P号页面在内存中的起始地址+页内偏移量W 3.8 具有快表的地址变换机构快表TLB：又称联想寄存器，是访问速度比内存快很多的高速缓存。 快表为什么速度快：直接从快表中取出该页对应的内存块号，再将内存块号与页内偏移量拼接形成物理地址最后，访问该物理地址对应的内存单元。因此，若快表命中，则访问某个逻辑地址仅需一次访存即可。 慢表则要在多级页表中找到对应的页表，再从相应的页号中取出内存块号。 就像看一本书，快表放了书签，而慢表则要慢慢找上次看到了哪 局部性原理： 时间局部性：如果执行了程序中的某条指令，那么不久后这条指令很有可能再次执行；如果某个数据被访问过，不久之后该数据很可能再次被访问。（因为程序中存在大量的循环） 空间局部性：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也很有可能被访问。（因为很多数据在内存中都是连续存放的） 3.9 两级页表为什么要有两级页表 根据局部性原理，进程某个时间段内只需要访问几个页面就可以正常运行了，没必要让整个页面都在内存中常驻。同时，过大的页表还会占用很多页框，也就是内存空间。 示意图： 当整个页表被分为多个页表，在内存中只需要放入有需求的页表，从而提高效率，节省空间。 3.10 基本分段管理 分段管理：以段为单位进行分配，每个段在内存中占据连续空间，各段之间可以不相邻。大体上与分页类似，与“分页”最大的区别就是——离散分配时所分配地址空间的基本单位不同。 3.11 虚拟内存在真实的操作系统中，通常采用段页式存储管理，段面向用户，页面向硬件。 虚拟内存解决的问题： 一次性：作业必须一次性全部装入内存后才能开始运行。这会造成两个问题：①作业很大时，不能全部装入内存，导致 大作业无法运行 ；②当大量作业要求运行时，由于内存 无法容纳所有作业 ，因此只有少量作业能运行，导致多道程序并发度下降。 驻留性：一旦作业被装入内存，就会一直驻留在内存中，直至作业运行结束。事实上，在一个时间段内，只需要访问作业的一小部分数据即可正常运行，这就导致了内存中会驻留大量的、暂时用不到的数据， 浪费了宝贵的内存资源。 虚拟内存的实现： 基于局部性原理，在程序装入时，可以将程序中很快会用到的部分装入内存，暂时用不到的部分 留在外存 ，就可以让程序开始执行。 在程序执行过程中，当所访问的信息不在内存时，由操作系统负责将所需信息从 外存调入内存 ，然后继续执行程序。 若内存空间不够，由操作系统负责将内存中暂时用不到的信息换出到外存。在操作系统的管理下，在用户看来似乎有一个比实际内存大得多的内存，这就是 虚拟内存 。 3.12 请求分页管理请求分页管理： 在程序执行过程中，当所访问的信息不在内存时，由操作系统负责将所需信息从外存调入内存，然后继续执行程序。若内存空间不够，由操作系统负责将内存中暂时用不到的信息换出到外存 缺页中断： 在请求分页系统中，每当要访问的页面不在内存时，便产生一个缺页中断，然后由操作系统的缺页中断处理程序处理中断。 缺页的进程阻塞，放入阻塞队列，调页完成后再将其唤醒，放回就绪队列。如果内存中有空闲块，则为进程分配一个空闲块，将所缺页面装入该块，并修改页表中相应的页表项。如果内存中没有空闲块，则由页面置换算法 选择一个页面淘汰，同时要注意，若某个页面被换出外存，则 快表 中的相应表项也要删除，否则可能访问错误的页面 理解缺页 ，缺页就像货架上缺少了商品，需要从仓库里调取商品，就先暂停这个货架的销售，等商品调取完毕再重新出售。 3.13 页面置换算法置换算法的评价指标是：缺页的次数，某种算法让缺页次数最低，调度效率最高，那就是最优的算法 最佳置换算法 每次淘汰的页面都是以后永久不用或最长时间不使用的页面，保证最低的缺页率。 显然，这种需要预测未来的算法不可能实现。 先进先出算法FIFO 缺页时，淘汰最早进入的页面。算法简单，但局限性也明显，例如某些经常使用的页面一直被换进换出，和使用频率低的页面有相同的被换出的机会。 最近最久未使用置换算法LRU 每次淘汰的页面都是最近最久未使用的页面。需要在页面中添加一个记录项，记录上次被访问以来经历的时间t，当需要淘汰页面时，选择时间t最大的淘汰，也就是最久未使用的淘汰。算法设计虽好，但开销很大，实现困难。 时钟置换算法 时钟置换算法也可以称为最近未使用算法。 是一种性能和开销均衡的算法。 简单的时钟算法实现方法:为每个页面设置一个访问位，再将内存中的页面都通过链接指针链接成一个循环队列。当某页被访问时，其访问位置为1。当需要淘汰一个页面时，只需检查页的访问位。如果是0，就选择该页换出;如果是1，则将它置为0，暂不换出，继续检查下一个页面，若第一轮扫描中所有页面都是1，则将这些页面的访问位依次置为0后，再进行第二轮扫描 （第二轮扫描中一定会有访问位为0的页面，因此简单的CLOCK算法选择一个淘汰页面最多会经过两轮扫描) 如下图。 了解即可 问题：缺页很少，访问位都是一，那么扫描了一轮进行第二次扫描，就相当于FIFO算法，为了改进这种情况，提出了下面的算法。 改进版的时钟算法(有很多改进方式，这里只是一种)：如果把页面看成钟表，原来的慢指针相当于时针，现在添加一个快指针，相当于分针，分针扫描要比时针快，如果一个页面在一定时间限制内没有访问，就设访问位为0，缺页时直接换出。 四、I/O原理 注意文件索引和文件目录结构的区别，索引是检索文件的方式，而文件目录结构是用户管理计算机文件的一种结构，单级目录、多级目录等等。在用户目录的基础上，文件控制块FCB会指向对应的索引块。 4.1 文件的逻辑结构 【了解即可】文件可以分为两类： 无结构文件 文件内部数据就是一系列二进制流或字符流。最典型的就是txt文件。 有结构文件 由一组相似的记录组成，又称记录式文件。典型的excel表、数据库表等。 有结构文件的逻辑结构又分顺序文件、索引文件、索引顺序文件，注意逻辑结构是展示给用户的，是文件的组织形式，例如是一张顺序存储的excel表格，还是一张excel索引表加上excel顺序表，还是多级索引加顺序，而不是在计算机上的存储方式。 顺序文件：文件中的记录一个接一个地顺序排列（逻辑上），记录可以是定长的或可变长的。各个记录在物理上可以顺序存储或链式存储。 顺序存储即逻辑相邻的文件物理上也相邻，链式存储即在末尾添加新的文件。 记录的类型又分为可变长和不可变长记录： 问题 ：对于可变长记录文件，要找到第i 个记录，必须先顺序第查找前 i - 1 个记录，但是很多应用场景中又必须使用可变长记录。如何解决这个问题？ 之后提出了索引文件:简单而言就是引入了索引表的文件。索引表本身是顺序文件，即索引表顺序存储在一起。 问题：：每个记录对应一个索引表项，因此索引表可能会很大。比如：文件的每个记录平均只占8B，而每个索引表项占32个字节，那么索引表都要比文件内容本身大4倍，这样对存储空间的利用率就太低了。 索引顺序文件：索引顺序文件是索引文件和顺序文件思想的结合。索引顺序文件中，同样会为文件建立一张索引表，但不同的是：并不是每个记录对应一个索引表项，而是一组记录对应一个索引表项，索引表项的地址直接指向顺序文件所在区域，再顺序查找到所需的文件，从而节省了很大的空间。(例如我们可以通过AnQi找到An Kang、An Jie等，而不用在索引表中存放这么多信息。另外索引项之间不需要有按照逻辑关系排列) 多级索引顺序文件：在索引顺序文件的基础上再增加层次深度，可以减少查找的次数(顺序查找范围缩小了) 4.2 文件目录文件目录可以分为：单级目录结构、两级目录结构、多级目录结构(树形目录结构) 单极目录结构：顾名思义，所有的文件放在一个目录中，类似于一个仓库把所有文件不加整理的堆放在一起，显然效率会很低下。 两级目录结构:主要分为主文件目录和用户文件目录。类似于仓库中加了几个员工货架，不同员工的货物放在不同货架，但在一个货架中文件还是采用堆砌式的存储。 多级目录结构，又称树形目录结构:我们当前主流操作系统都是多级目录结构，简而言之就是文件目录可以一级一级的延申，从而文件更有条理。 FCB(文件控制块)，首先来看一张图，如果文件目录都以这种表的形式进行信息查找，会大大降低运行效率，增加系统负担。 提出对策，其实在查找各级目录的过程中，只需要用到文件名这个信息，可以考虑让目录表瘦身来提升效率。 索引结点指针指向索引结点(文件名之外的其他信息就存放在结点中，从而按需读取，提升效率) FCB 每一个文件都有一个FCB，记录了文件的地址、信息、权限等等属性 4.3 文件的物理结构【重要】最重要的三种物理结构：顺序、链接、索引，其中最主要使用的是索引文件，可以随机访问，同时增删效率高。 思考：想想数据结构中，顺序表和链表分别的优缺点是什么? 文件的物理结构是文件分配在计算机存储上的分配方式。分配的基本单位是物理块,可以构想一下，一个大文件，如一首音乐23MB，难道直接一整个塞入硬盘吗？显然可能会出现一些问题，硬盘的空间也需要不断调整，就像内存分页一样，硬盘也被分为小的物理块号方便进行调度。 连续分配 优点：支持顺序访问和直接访问（即随机访问）；连续分配的文件在顺序访问时速度最快 缺点：不方便文件拓展；存储空间利用率低，会产生磁盘碎片 链式分配 隐式 采用链式分配（隐式链接）方式的文件，只支持顺序访问，不支持随机访问，查找效率低。另外，指向下一个盘块的指针也需要耗费少量的存储空间。 显式 直观理解就是在隐式的基础上添加了一张表，从表上能看出不同物理块号的下一块的地址 结论：采用链式分配（显式链接）方式的文件，支持顺序访问，也支 持随机访问（想访问i 号逻辑块时，并不需要依次访问之前的0 ~ i-1 号逻辑块），由于块号转换的过程不需要访问磁盘，因此相比于隐式 链接来说，访问速度快很多。 显然，显式链接也不会产生外部碎片，也可以很方便地对文件进行拓 展。 索引分配 单级 多级 索引就是文件分成不同的物理块存入磁盘，对每个物理块都有一个索引与之对应，需要读写时就通过索引表查询其物理地址进行相关操作。 4.3 磁盘结构 磁道：每一圈就是一个磁道，最内侧磁道面积最小，所以数据密度最大 扇区：磁道被划分为小的磁盘块 一个盘片可能有两个盘面;每个盘面对应一个磁头；所有磁头连在一起，共进退；每个盘面的相对位置的磁道组成柱面 两种类型的磁盘： 4.4 磁盘调度算法磁盘调度算法要解决的核心问题就是 寻道时间 ，即移动磁头的时间，而其他的启动时间、传输时间都很迅速，不是最主要的时间消耗。 这里讲三种算法： 先来先服务FCFS 根据进程请求房屋内磁盘的现后顺序进行调度。符合惯性思维，但在很多时候，效果很差。 最短寻找时间优先(学过数据结构与算法的话，核心思想就是贪心算法)，该算法会优先处理与当前磁头最近的磁道的需求。 那么很可能磁头就会如图所示的移动，也会存在饥饿问题：磁头只在一个小区域移动，而不能满足需要远距离移动的需求。例如不断有18-&gt;38，38-&gt;18的需求，那磁头就不会执行18-&gt;150的请求，从而产生饥饿。 扫描算法 核心思想，只有磁头移动到最外侧磁道的时候才能往内侧移动，移动到最内侧的时候才能向外侧移动。 这样就不会产生饥饿问题。 4.5 文件共享文件共享分两种链接方式，硬链接和软连接 硬链接就是在另一个用户的目录中，索引结点指针直接指向了发送分享的用户的索引节点，从而实现了共享，count的数量代表文件正在被几个用户使用。 软连接，类似于快捷方式，记录了原文件的路径，然后层层查找。 例如文件2的内容是 C:/User1/aaa，也就是要去User1中寻找aaa的文件名对应的索引结点。 4.6 文件保护文件保护有三种方式口令、加密、访问控制 口令 为文件设置一串口令，就像打开手机需要先解锁。 加密 使用加密方法对文件加密，只有拥有正确的解密方法才能解密，有点像不同军队之间进行通信，要实现进行加密，要是想窥探敌情，就要对密文进行破解。 访问控制 每个文件的FCB或者索引结点中设置访问控制表，如windows中，设置了很多的访问权限，例如 4.7 I/O设备I/O就是输入输出，I/O设备就是可以将数据输入到计算机或将计算机数据输出的设备，常见的：鼠标、键盘、音响、显示器、打印机、话筒、摄像头等等。 I/O控制器:CPU无法直接控制I/O设备，需要一个电子部件去充当中间人，这个部件就是I/O控制器，CPU控制I/O控制器，I/O控制器控制I/O设备。 假如我们的CPU能够控制I/O设备，那不同的厂商、不同型号的设备，都要对应进行编码，显然是不切实际的，所以CPU要采用通用调度方式调度I/O设备从而需要I/O控制器。 I/O控制器的组成 以下作为了解。 例如我们在Java语言中，调用System.out.Println()，这本身并不能在显示器上打印，而需要通过操作系统调用write方法，接着调用字符设备接口，命令显示器写。","link":"/Article/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0/"},{"title":"Mysql常见问题","text":"一些我使用Mysql时常遇到的小问题启动服务 1service mysql start &gt;/dev/null 2&gt;&amp;1 连不上虚拟机的mysql123456mysql -u root -pselect Host,User,plugin from mysql.user;update user set host = '%' where host = 'localhost';flush privileges; MySQL外网连接不上1vim /etc/mysql/mysql.conf.d/mysqld.cnf 1/etc/init.d/mysql restart 1create user '666'@'%' identified by '000000'; 1grant all privileges on *.* to '666'@'%' identified by '000000' with grant option; 1flush privileges; 1251 - Client does not support authentication protocol requested by server1alter user 'root'@'localhost' identified with mysql_native_password by '000000'; 1flush privileges; 一些碎碎念最近在忙着考研，摆烂了很久，没有更新。感觉像Hexo这样的静态博客还是不太适合我，本来就很少有写技术博客的习惯，加上Hexo是本地服务用的就更少了，还是微博跟即刻更新日常比较多，我在考虑要不要整个服务器玩玩了，部署一些服务，顺便整个博客来发发颠。下一篇写tailscale的教程，有一说一Tailscale够用但是已经不能满足我的需求了，主要是用的家里的台式，不太敢乱来，哎，不知道下一篇又会被我推到什么时候了。","link":"/Article/Mysql%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"ADB","slug":"ADB","link":"/tags/ADB/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"PS4","slug":"PS4","link":"/tags/PS4/"},{"name":"Games","slug":"Games","link":"/tags/Games/"}],"categories":[{"name":"数据采集","slug":"数据采集","link":"/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"},{"name":"Termux","slug":"Termux","link":"/categories/Termux/"},{"name":"期末复习","slug":"期末复习","link":"/categories/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"Diary","slug":"Diary","link":"/categories/Diary/"}],"pages":[{"title":"关于作者","text":"一个平平无奇的大三学生 Weibo：Nameless0322Github：Nameless1732Mail：1727841610@qq.com","link":"/about/index.html"}]}