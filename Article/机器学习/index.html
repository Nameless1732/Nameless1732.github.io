<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>机器学习复习 - Nameless1732</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Nameless1732"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Nameless1732"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="1、k近邻算法：使用sklearn中的相关函数实现鸢尾花案例的k近邻分类，尝试使用不同的k值以比较实验结果。最后，请用以下函数实现K值自动调优。（交叉验证，网格搜索（模型选择与调优）API：sklearn.model_selection.GridSearchCV(estimator, param_grid&amp;#x3D;None,cv&amp;#x3D;None) 2、决策树算法：如图数据集，住房（1表示拥"><meta property="og:type" content="blog"><meta property="og:title" content="机器学习复习"><meta property="og:url" content="http://nameless1732.github.io/Article/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><meta property="og:site_name" content="Nameless1732"><meta property="og:description" content="1、k近邻算法：使用sklearn中的相关函数实现鸢尾花案例的k近邻分类，尝试使用不同的k值以比较实验结果。最后，请用以下函数实现K值自动调优。（交叉验证，网格搜索（模型选择与调优）API：sklearn.model_selection.GridSearchCV(estimator, param_grid&amp;#x3D;None,cv&amp;#x3D;None) 2、决策树算法：如图数据集，住房（1表示拥"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-0.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-1.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-2.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-3.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-8.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-9.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-10.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-11.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-12.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-13.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-14.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-4.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-5.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-6.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-16.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-17.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-18.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-19.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-20.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-21.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-22.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-23.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-24.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-25.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-26.png"><meta property="og:image" content="http://nameless1732.github.io/images/machinelearning-27.png"><meta property="article:published_time" content="2022-11-17T07:37:54.000Z"><meta property="article:modified_time" content="2023-04-18T21:51:04.673Z"><meta property="article:author" content="Jie"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://nameless1732.github.io/images/machinelearning-0.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://nameless1732.github.io/Article/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},"headline":"机器学习复习","image":["http://nameless1732.github.io/images/machinelearning-0.png","http://nameless1732.github.io/images/machinelearning-1.png","http://nameless1732.github.io/images/machinelearning-2.png","http://nameless1732.github.io/images/machinelearning-3.png","http://nameless1732.github.io/images/machinelearning-8.png","http://nameless1732.github.io/images/machinelearning-9.png","http://nameless1732.github.io/images/machinelearning-10.png","http://nameless1732.github.io/images/machinelearning-11.png","http://nameless1732.github.io/images/machinelearning-12.png","http://nameless1732.github.io/images/machinelearning-13.png","http://nameless1732.github.io/images/machinelearning-14.png","http://nameless1732.github.io/images/machinelearning-4.png","http://nameless1732.github.io/images/machinelearning-5.png","http://nameless1732.github.io/images/machinelearning-6.png","http://nameless1732.github.io/images/machinelearning-16.png","http://nameless1732.github.io/images/machinelearning-17.png","http://nameless1732.github.io/images/machinelearning-18.png","http://nameless1732.github.io/images/machinelearning-19.png","http://nameless1732.github.io/images/machinelearning-20.png","http://nameless1732.github.io/images/machinelearning-21.png","http://nameless1732.github.io/images/machinelearning-22.png","http://nameless1732.github.io/images/machinelearning-23.png","http://nameless1732.github.io/images/machinelearning-24.png","http://nameless1732.github.io/images/machinelearning-25.png","http://nameless1732.github.io/images/machinelearning-26.png","http://nameless1732.github.io/images/machinelearning-27.png"],"datePublished":"2022-11-17T07:37:54.000Z","dateModified":"2023-04-18T21:51:04.673Z","author":{"@type":"Person","name":"Jie"},"publisher":{"@type":"Organization","name":"Nameless1732","logo":{"@type":"ImageObject","url":"http://nameless1732.github.io/img/logo.svg"}},"description":"1、k近邻算法：使用sklearn中的相关函数实现鸢尾花案例的k近邻分类，尝试使用不同的k值以比较实验结果。最后，请用以下函数实现K值自动调优。（交叉验证，网格搜索（模型选择与调优）API：sklearn.model_selection.GridSearchCV(estimator, param_grid&#x3D;None,cv&#x3D;None) 2、决策树算法：如图数据集，住房（1表示拥"}</script><link rel="canonical" href="http://nameless1732.github.io/Article/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css"><link rel="stylesheet"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Nameless1732" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">文章</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Nameless1732"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-11-17T07:37:54.000Z" title="11/17/2022, 3:37:54 PM">2022-11-17</time>发表</span><span class="level-item"> Jie </span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">30 分钟读完 (大约4540个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习复习</h1><div class="content"><blockquote>
<p>1、k近邻算法：<br>使用sklearn中的相关函数实现鸢尾花案例的k近邻分类，尝试使用不同的k值以比较实验结果。最后，请用以下函数实现K值自动调优。<br>（交叉验证，网格搜索（模型选择与调优）API：sklearn.model_selection.GridSearchCV(estimator, param_grid&#x3D;None,cv&#x3D;None)</p>
<p>2、决策树算法：<br>如图数据集，住房（1表示拥有住房，0表示没有住房）；婚姻（0表示单身，1表示已婚，2表示离异）；年收入一栏中单位为1000元；（拖欠贷款一栏0表示不拖欠，1表示拖欠）</p>
</blockquote>
<span id="more"></span>
<p><strong>任务：使用sklearn的tree函数、 graphviz函数，选择合适的算法，构造并绘制决策树</strong></p>
<table>
<thead>
<tr>
<th><strong>HOUSE</strong></th>
<th><strong>Marriage</strong></th>
<th><strong>Annual  salary</strong></th>
<th><strong>No  default&#x2F;Default</strong></th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0</td>
<td>125</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>100</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>70</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>120</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>2</td>
<td>95</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>60</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>2</td>
<td>220</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>85</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>75</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>90</td>
<td>1</td>
</tr>
</tbody></table>
<p><strong>k近邻算法</strong></p>
<p>利用sklearn中自带的鸢尾花数据集，并通过KNN算法实现对鸢尾花的分类，使用不同的k值比较实验结果，最后使用以下函数实现K值自动调优。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)</span><br><span class="line">estimator：估计器对象</span><br><span class="line">param_grid：估计器参数(dict)&#123;“n_neighbors”:[1,3,5,10]&#125;</span><br><span class="line">cv：指定几折交叉验证</span><br></pre></td></tr></table></figure>

<p><strong>关键代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.获取鸢尾花的特征值，目标值</span></span><br><span class="line">iris_data, iris_target = self.get_iris_data()</span><br><span class="line"><span class="comment"># 2.将数据分割成训练集和测试集 test_size=0.30表示将30%的数据用作测试集 random_state=0可以将数据打散后再分割</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_target, random_state=<span class="number">0</span>) </span><br><span class="line"><span class="comment"># 将数据随机打散后可以看到准确率可以达到 97%</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_target, test_size=<span class="number">0.30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.特征工程(对特征值进行标准化处理)</span></span><br><span class="line">std = StandardScaler()</span><br><span class="line">x_train = std.fit_transform(x_train)</span><br><span class="line">x_test = std.transform(x_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.送入算法</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;使用不同k制进行预测&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">    knn.fit(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(k, <span class="string">&quot;准确率：&quot;</span>, knn.score(x_test, y_test))</span><br><span class="line"><span class="comment"># 5.使用GridSearchCV函数实现K值自动调优</span></span><br><span class="line"><span class="comment"># 生成knn估计器</span></span><br><span class="line">knn = KNeighborsClassifier()</span><br><span class="line"><span class="comment"># 构造超参数值</span></span><br><span class="line">params = &#123;<span class="string">&quot;n_neighbors&quot;</span>: [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>]&#125;</span><br><span class="line"><span class="comment"># 进行网格搜索</span></span><br><span class="line">gridCv = GridSearchCV(knn, param_grid=params, cv=<span class="number">25</span>)</span><br><span class="line">gridCv.fit(x_train, y_train)  <span class="comment"># 输入训练数据</span></span><br><span class="line"><span class="comment"># 预测准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率：&quot;</span>, gridCv.score(x_test, y_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;交叉验证中最好的结果：&quot;</span>, gridCv.best_score_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最好的模型：&quot;</span>, gridCv.best_estimator_)</span><br></pre></td></tr></table></figure>

<p><strong>运行截图</strong><br><img src="/images/machinelearning-0.png"><br><img src="/images/machinelearning-1.png"><br><img src="/images/machinelearning-2.png"><br><img src="/images/machinelearning-3.png"></p>
<p><strong>决策树算法</strong></p>
<p>首先要选择最优的划分属性，尽量使划分的样本属于同一类别，即“纯度”最高的属性。定义数据的“纯度”就成了问题的关键，主流的算法是CART算法，sklearn中DecisionTreeClassifier就是用的CART算法，其判别纯度的指标是entropy熵与Gini指数。</p>
<p>熵的公式为：<img src="/images/machinelearning-8.png"></p>
<p>K表示有K种类别，pi是X取第i种类的概率，实际计算直接用第i种类别所占的比例代替，熵越大，表示此时的混乱程度越大，数据越不纯。</p>
<p>如果利用某一种指标A&#x3D;a将原数据D分为两类D1与D2之后，在这个分类下，定义此时的条件熵为：<img src="/images/machinelearning-9.png"></p>
<p>定义此时的基尼指数为：<img src="/images/machinelearning-10.png"></p>
<p>CART算法生成的决策树是二叉树，每一步只对某一个指标做出划分。如果特征是离散的取值，那么就对每个特征的每个不同的取值作为二叉树的判定标准，大于或者小于等于该值分成两类，并计算以该点分类后，新节点的信息增益或者基尼指数，以两个子节点的样本数占比进行加权计算；如果特征是连续的取值，那么以每两个相邻的特征取值的算术平均离散化。</p>
<p>设需要分类的数据为D，定义数据集内数据量为D，属性集S，算法的<strong>伪代码</strong>如下：</p>
<p>① 设定纯度判断阈值<img src="/images/machinelearning-11.png">与样本数量阈值<img src="/images/machinelearning-12.png">等防止树过深的参数</p>
<p>② 若<img src="/images/machinelearning-13.png">)或<img src="/images/machinelearning-14.png">或没有特征，则停止，返回当前树，否则转③</p>
<p>③根据所有特征的值进行二分类，选出基尼指数最小或信息增益最大的那个特征的值作为分类依据，得到的两个子数据集D1,D2 并返回②</p>
<p><strong>关键代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tree_model = tree.DecisionTreeClassifier(criterion=<span class="string">&#x27;gini&#x27;</span>,</span><br><span class="line">                                         max_depth=<span class="literal">None</span>,</span><br><span class="line">                                         min_samples_leaf=<span class="number">1</span>,</span><br><span class="line">                                         ccp_alpha=<span class="number">0.0</span>)</span><br><span class="line">tree_model.fit(X, y)</span><br><span class="line">dot_data = StringIO()</span><br><span class="line">feature_names = [<span class="string">&#x27;House&#x27;</span>, <span class="string">&#x27;Marriage&#x27;</span>, <span class="string">&#x27;Annual salary&#x27;</span>]</span><br><span class="line">target_names = [<span class="string">&#x27;No default&#x27;</span>, <span class="string">&#x27;Default&#x27;</span>]</span><br><span class="line">tree.export_graphviz(tree_model,</span><br><span class="line">                     out_file=dot_data,</span><br><span class="line">                     feature_names=feature_names,</span><br><span class="line">                     class_names=target_names,</span><br><span class="line">                     filled=<span class="literal">True</span>,</span><br><span class="line">                     rounded=<span class="literal">True</span>,</span><br><span class="line">                     special_characters=<span class="literal">True</span>)</span><br><span class="line">dot_data = dot_data.getvalue()</span><br><span class="line">dot_data = dot_data.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">graph = pydotplus.graph_from_dot_data(dot_data.getvalue())</span><br><span class="line">graph.write_pdf(<span class="string">&quot;default.pdf&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/images/machinelearning-4.png"><br><img src="/images/machinelearning-5.png"><br><img src="/images/machinelearning-6.png"></p>
<p><strong>20Newsgroups文档分类</strong></p>
<p>基于20Newsgroups数据集，利用朴素贝叶斯算法（请尝试使用不同的分类器，如GaussianNB、MultinominalNB等），对数据集进行文本分类，结合交叉验证，对分类结果进行简要阐释。</p>
<p><strong>下载数据集</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">news = fetch_20newsgroups(subset=&#x27;all&#x27;)</span><br></pre></td></tr></table></figure>

<p>文本数据属于非结构化的数据，一般要转换成结构化的数据才能够通过机器学习算法进行分类。常见的做法是将文本转换成“文档-词项矩阵”，矩阵中的元素可以使用词频或TF-IDF值等。</p>
<p>首先分割训练数据和测试数据，然后采用普通统计CountVectorizer提取特征向量，然后再采用TfidfVectorizer提取文本特征向量，然后实例化一个朴素贝叶斯分类器，并将我们的训练集特征与训练集特征对应的分类结果导入到模型中并训练模型，传入测试集，进行预测，并打印预测结果与预测精度。</p>
<p><strong>关键代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分割训练数据和测试数据</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用普通统计CountVectorizer提取特征向量</span></span><br><span class="line">count_vec = CountVectorizer()</span><br><span class="line">x_count_train = count_vec.fit_transform(x_train)</span><br><span class="line">x_count_test = count_vec.transform(x_test) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用TfidfVectorizer提取文本特征向量</span></span><br><span class="line">tfid_vec = TfidfVectorizer()</span><br><span class="line">x_tfid_train = tfid_vec.fit_transform(x_train)</span><br><span class="line">x_tfid_test = tfid_vec.transform(x_test)</span><br><span class="line"></span><br><span class="line">mnb_count = MultinomialNB()</span><br><span class="line">mnb_count.fit(x_count_train, y_train)</span><br><span class="line">mnb_count_y_predict = mnb_count.predict(x_count_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试集的预测结果为：&quot;</span>, mnb_count_y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率：&quot;</span>, mnb_count.score(x_count_test, y_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;详细的评估指标:\n&quot;</span>, classification_report(mnb_count_y_predict, y_test))</span><br><span class="line"></span><br><span class="line">mnb_tfid = MultinomialNB()</span><br><span class="line">mnb_tfid.fit(x_tfid_train, y_train)</span><br><span class="line">mnb_tfid_y_predict = mnb_tfid.predict(x_tfid_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试集的预测结果为：&quot;</span>, mnb_tfid_y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率：&quot;</span>, mnb_tfid.score(x_tfid_test, y_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;详细的评估指标:\n&quot;</span>, classification_report(mnb_tfid_y_predict, y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置评估算法的基准</span></span><br><span class="line">num_folds = <span class="number">10</span></span><br><span class="line">seed = <span class="number">7</span></span><br><span class="line">scoring = <span class="string">&#x27;accuracy&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证 自动调参</span></span><br><span class="line">param_grid = &#123;<span class="string">&#x27;alpha&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1.5</span>]&#125;</span><br><span class="line">model = MultinomialNB()</span><br><span class="line">kfold = KFold(n_splits=num_folds, shuffle=<span class="literal">True</span>, random_state=seed)</span><br><span class="line">grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)</span><br><span class="line">grid_result = grid.fit(X=x_tfid_train, y=y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最优：%s 使用%s&#x27;</span> % (grid_result.best_score_, grid_result.best_params_))</span><br></pre></td></tr></table></figure>
<p><strong>运行截图</strong><br><img src="/images/machinelearning-16.png"><br><img src="/images/machinelearning-17.png"><br><img src="/images/machinelearning-18.png"></p>
<p><strong>逻辑回归与梯度下降</strong></p>
<p>1、 根据小批量梯度下降MBGD的原理，写出python函数代码。</p>
<p>2、 使用MBGD实现患疝气病的马存活问题的Logistic回归来预测</p>
<blockquote>
<p>改变batch_size的大小，比较模型表现，选择最优batch_size<br>将预测结果与BGD、SGD的结果进行比较与讨论（准确度、收敛速度等）</p>
</blockquote>
<p>小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用batch_size个样本来对参数进行更新。假设 batchsize&#x3D;10，样本数 m&#x3D;1000。<br>伪代码形式为：<img src="/images/machinelearning-19.png"></p>
<p><strong>关键代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">MBGD</span>(<span class="params">dataSet, alpha=<span class="number">0.001</span>, batch_size=<span class="number">10</span>, sample=<span class="number">368</span></span>):</span><br><span class="line">    xMat = np.mat(dataSet.iloc[:, :-<span class="number">1</span>].values)</span><br><span class="line">    yMat = np.mat(dataSet.iloc[:, -<span class="number">1</span>].values).T</span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    m, n = xMat.shape</span><br><span class="line">    weights = np.zeros((n, <span class="number">1</span>))</span><br><span class="line">    m = <span class="built_in">int</span>(sample / batch_size)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;batch_size =&quot;</span>, batch_size, end=<span class="string">&quot; &quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            grad = xMat.T * (xMat * weights - yMat) / batch_size</span><br><span class="line">            weights = weights - alpha * grad</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_acc</span>(<span class="params">train, test, alpha=<span class="number">0.001</span>, maxCycles=<span class="number">5000</span>, batch_size=<span class="number">10</span>, sample=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="comment"># weights = SGD_LR(train, alpha=alpha, maxCycles=maxCycles)</span></span><br><span class="line">    weights = MBGD(train, alpha=alpha, batch_size=batch_size, sample=sample)</span><br><span class="line">    xMat = np.mat(test.iloc[:, :-<span class="number">1</span>].values)</span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> inX <span class="keyword">in</span> xMat:</span><br><span class="line">        label = classify(inX, weights)</span><br><span class="line">        result.append(label)</span><br><span class="line">    retest = test.copy()</span><br><span class="line">    retest[<span class="string">&#x27;predict&#x27;</span>] = result</span><br><span class="line">    acc = (retest.iloc[:, -<span class="number">1</span>] == retest.iloc[:, -<span class="number">2</span>]).mean()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;模型准确率为：<span class="subst">&#123;acc&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> retest</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">35</span>):</span><br><span class="line">    get_acc(train, test, alpha=<span class="number">0.001</span>, batch_size=i)</span><br></pre></td></tr></table></figure>

<p><img src="/images/machinelearning-20.png"><br><img src="/images/machinelearning-21.png"><br><img src="/images/machinelearning-22.png"><br><strong>由结果可知，最优batch_size为21</strong></p>
<p><strong>对比BGD与SGD：</strong><br>BGD每一次迭代时使用所有样本来进行梯度的更新，SGD是每次迭代使用一个样本来对参数进行更新，对于BGD而言，每次迭代需要计算所有样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设10次）。对于SGD，每次更新参数只需要一个样本。假设样本有30W个，若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。在收敛时，BGD计算了10×30W次，而SGD只计算了1×30W次。从迭代的次数上来看，SGD迭代的次数较少，在解空间的搜索过程看起来很盲目。而MBGD使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。（设置batch_size&#x3D;100时，只需要迭代3000次）</p>
<p><strong>总结</strong><br>Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。MBGD通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多，每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果，且可实现并行化。</p>
<p><strong>支持向量机</strong></p>
<p>1、 使用Iris原始数据集，构建多分类SVM模型</p>
<blockquote>
<p>尝试改变各种超参数，特别是多分类参数decision_function_shape，比较模型结果是否存在差异<br>输出各类的支持向量的个数<br>自主探究对于多分类问题，如何可视化分类结果</p>
</blockquote>
<p>2、 手写数字分类</p>
<p>比较使用决策树、朴素贝叶斯及支持向量机同样实现手写数字分类的效果的不同，简述三种方法各自的优劣</p>
<p><strong>sklearn.svm.SVC 支持向量机参数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.svm.SVC(*, C=<span class="number">1.0</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;scale&#x27;</span>, coef0=<span class="number">0.0</span>, shrinking=<span class="literal">True</span>, probability=<span class="literal">False</span>, tol=<span class="number">0.001</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, verbose=<span class="literal">False</span>, max_iter=-<span class="number">1</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, break_ties=<span class="literal">False</span>, random_state=<span class="literal">None</span>)</span><br><span class="line">•	C：正则化参数。正则化的强度与C成反比。必须严格为正。惩罚是平方的l2惩罚。(默认<span class="number">1.0</span>)， 惩罚参数越小，容忍性就越大</span><br><span class="line">•	kernel：核函数类型，可选‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’</span><br><span class="line">•	degree：当选择核函数为poly多项式时，表示多项式的阶数。</span><br><span class="line">•	gamma：可选‘scale’和‘auto’，表示为“ rbf”，“ poly”和“ Sigmoid”的内核系数。默认是<span class="string">&#x27;scale&#x27;</span>,gamma取值为<span class="number">1</span> / (n_features * X.var())；当选‘auto’参数时gamma取值为<span class="number">1</span> / n_features。</span><br><span class="line">•	decision_function_shape：多分类的形式，<span class="number">1</span> vs 多(‘ovo’)还是<span class="number">1</span> vs <span class="number">1</span>(’ovr’)，默认’ovr’</span><br><span class="line">•	probability：是否启用概率估计,默认是<span class="literal">False</span>。必须在调用fit之前启用此功能，因为该方法内部使用<span class="number">5</span>倍交叉验证，因而会减慢该方法的速度，并且predict_proba可能与<span class="built_in">dict</span>不一致。</span><br><span class="line">•	max_iter：算法迭代的最大步数，默认-<span class="number">1</span>表示无限制</span><br><span class="line">•	random_state：随机种子，随机打乱样本。</span><br></pre></td></tr></table></figure>

<p><strong>完整代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]  <span class="comment"># 取前两维特征</span></span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出数据的散点图来看看分布状况</span></span><br><span class="line">plt.scatter(X[y == <span class="number">0</span>, <span class="number">0</span>], X[y == <span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.scatter(X[y == <span class="number">1</span>, <span class="number">0</span>], X[y == <span class="number">1</span>, <span class="number">1</span>], color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span>)</span><br><span class="line">plt.scatter(X[y == <span class="number">2</span>, <span class="number">0</span>], X[y == <span class="number">2</span>, <span class="number">1</span>], color=<span class="string">&#x27;g&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y1&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型的训练</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># C是误差惩罚系数，用来调节模型方差与偏差的问题</span></span><br><span class="line">C = <span class="number">1.0</span></span><br><span class="line">lin_svc = svm.SVC(decision_function_shape=<span class="string">&#x27;ovo&#x27;</span>, kernel=<span class="string">&#x27;linear&#x27;</span>, C=C).fit(X_train, y_train)  <span class="comment"># 线性核</span></span><br><span class="line">rbf_svc = svm.SVC(decision_function_shape=<span class="string">&#x27;ovo&#x27;</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="string">&#x27;auto&#x27;</span>, C=C).fit(X_train, y_train)  <span class="comment"># 径向基核</span></span><br><span class="line">poly_svc = svm.SVC(decision_function_shape=<span class="string">&#x27;ovo&#x27;</span>, kernel=<span class="string">&#x27;poly&#x27;</span>, degree=<span class="number">3</span>, C=C).fit(X_train, y_train)  <span class="comment"># 多项式核</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;支持向量个数：&quot;</span>,<span class="built_in">len</span>(lin_svc.support_vectors_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;支持向量个数：&quot;</span>,<span class="built_in">len</span>(rbf_svc.support_vectors_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;支持向量个数：&quot;</span>,<span class="built_in">len</span>(poly_svc.support_vectors_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型的评估</span></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># 网格中的步长</span></span><br><span class="line">x_min, x_max = X_train[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X_train[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">y_min, y_max = X_train[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, X_train[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                     np.arange(y_min, y_max, h))</span><br><span class="line"></span><br><span class="line">titles = [<span class="string">&#x27;LinearSVC (linear kernel)&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;SVC with RBF kernel&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;SVC with polynomial (degree 3) kernel&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>((lin_svc, rbf_svc, poly_svc)):</span><br><span class="line">    <span class="comment"># 绘出决策边界，不同的区域分配不同的颜色</span></span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">2</span>, i + <span class="number">1</span>)  <span class="comment"># 创建一个2行2列的图，并以第i个图为当前图</span></span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.4</span>, hspace=<span class="number">0.4</span>)  <span class="comment"># 设置子图间隔</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将xx和yy中的元素组成一对对坐标，作为支持向量机的输入，返回一个array</span></span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把分类结果绘制出来</span></span><br><span class="line">    Z = Z.reshape(xx.shape)  <span class="comment"># (220, 280)</span></span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired,</span><br><span class="line">                 alpha=<span class="number">0.8</span>)  <span class="comment"># 使用等高线的函数将不同的区域绘制出来</span></span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=plt.cm.Paired)  <span class="comment"># cmap相当于是配色盘</span></span><br><span class="line">    <span class="comment"># 将训练数据以离散点的形式绘制出来</span></span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Sepal length&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Sepal width&#x27;</span>)</span><br><span class="line">    plt.xlim(xx.<span class="built_in">min</span>(), xx.<span class="built_in">max</span>())</span><br><span class="line">    plt.ylim(yy.<span class="built_in">min</span>(), yy.<span class="built_in">max</span>())</span><br><span class="line">    plt.xticks(())</span><br><span class="line">    plt.yticks(())</span><br><span class="line">    plt.title(titles[i])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;linear&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(lin_svc.predict(X_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, lin_svc.score(X_test, y_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;rbf&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(rbf_svc.predict(X_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, rbf_svc.score(X_test, y_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;poly&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(poly_svc.predict(X_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, poly_svc.score(X_test, y_test))</span><br></pre></td></tr></table></figure>

<p><img src="/images/machinelearning-23.png"></p>
<p><strong>不同参数的影响</strong><br>1.	参数C和gamma都会影响支持向量的数量。参数C越大，支持向量的数量越少。<br>2.	当 gamma 很小时，每一个支持向量的影响范围很大，甚至可以包含整个训练集，从而使得模型受限于每一个数据点，并不能很好的反应数据的复杂性。这样结果就是模型会接近线性。当gamma 很大时，每一个支持向量的影响范围很小，导致C并不能很好地实现 regularization 从而无法避免过拟合。<br>3.	仅凭支持向量的数量，不能很好地说明模型是否存在过拟合（或者欠拟合）的问题。<br>4.	更少的支持向量占用的存储空间更小，预测速度更快。</p>
<p><strong>支持向量的个数</strong><br><img src="/images/machinelearning-24.png"></p>
<p><strong>手写数字分类</strong></p>
<p><strong>关键代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mnist = load_digits()</span><br><span class="line">x, test_x, y, test_y = train_test_split(mnist.data, mnist.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">40</span>)</span><br><span class="line">model = svm.LinearSVC(max_iter=<span class="number">10000</span>)</span><br><span class="line">model.fit(x, y)</span><br><span class="line">z = model.predict(test_x)</span><br></pre></td></tr></table></figure>

<p><strong>对比三种算法输出的结果</strong><br>1.	SVM<br>优点：适合小样本、非线性、高维模式识别。<br>缺点：对于大规模数据开销大，不合适多分类；对缺失数据敏感；需要选择适当的核函数。<br>2.	决策树<br>优点：简单易于理解，能够处理多路输出问题。<br>缺点：容易过拟合；决策树的生成不稳定，微小的数据变化可能导致生成的决策树不同。<br>3.	KNN<br>优点：简单易于理解，无需训练，无需估计参数准确性高；适合多标签问题。<br>缺点：懒惰算法，预测慢，开销大类的样本数不平衡时准确率受影响；可解释性差。</p>
<p><strong>运行截图</strong><br><img src="/images/machinelearning-25.png"><br><img src="/images/machinelearning-26.png"><br><img src="/images/machinelearning-27.png"></p>
<p><strong>总结</strong><br>SVM的主要思想是：建立一个超平面作为决策平面，使得正例和反例之间的隔离边缘被最大化。SVM也是结构风险最小化方法的近似实现。SVM是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机。由于在线性支持向量机学习的对偶问题里，目标函数和分类决策函数都只涉及实例和实例之间的内积，所以不需要显式地指定非线性变换，而是用核函数替换当中的内积。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>机器学习复习</p><p><a href="http://nameless1732.github.io/Article/机器学习/">http://nameless1732.github.io/Article/机器学习/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Jie</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2022-11-17</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2023-04-19</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Article/Mysql%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Mysql常见问题</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Article/Kafka%E5%85%A5%E9%97%A8/"><span class="level-item">Kafka入门</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/114732220?v=4" alt="Jie"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jie</p><p class="is-size-6 is-block">Web Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>广东广州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">12</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">5</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">5</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Nameless1732" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Nameless1732"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">四月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">十一月 2022</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">九月 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">六月 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Diary/"><span class="level-start"><span class="level-item">Diary</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Termux/"><span class="level-start"><span class="level-item">Termux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"><span class="level-start"><span class="level-item">数据采集</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"><span class="level-start"><span class="level-item">期末复习</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-18T21:19:47.000Z">2023-04-19</time></p><p class="title"><a href="/Article/Mysql%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">Mysql常见问题</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-11-17T07:37:54.000Z">2022-11-17</time></p><p class="title"><a href="/Article/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习复习</a></p><p class="categories"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-11-16T13:53:54.000Z">2022-11-16</time></p><p class="title"><a href="/Article/Kafka%E5%85%A5%E9%97%A8/">Kafka入门</a></p><p class="categories"><a href="/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/">数据采集</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-11-11T15:27:54.000Z">2022-11-11</time></p><p class="title"><a href="/Article/Scrapy%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/">Scrapy爬虫入门</a></p><p class="categories"><a href="/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/">数据采集</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-11-11T13:31:54.000Z">2022-11-11</time></p><p class="title"><a href="/Article/Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/">Python爬虫入门</a></p><p class="categories"><a href="/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/">数据采集</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ADB/"><span class="tag">ADB</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Android/"><span class="tag">Android</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Games/"><span class="tag">Games</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PS4/"><span class="tag">PS4</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Nameless1732" height="28"></a><p class="is-size-7"><span>&copy; 2023 Jie</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/Nameless1732"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>